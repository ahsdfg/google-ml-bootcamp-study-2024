{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LLM 20 Questions Starter Notebook\n",
    "> https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook"
   ],
   "id": "6866d0d9bd0a0592"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리와 GEMMA 모델을 설치합니다.\n",
    "\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "# 설정: 필요한 라이브러리를 임포트하고, 시스템 경로를 설정합니다.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "# **중요:** 코드가 노트북과 시뮬레이션 환경 양쪽에서 모두 작동하도록 시스템 경로를 이렇게 설정합니다.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 모델 가중치의 경로를 설정합니다. 시뮬레이션 환경에서 실행 중이면 KAGGLE_AGENT_PATH를 사용하고,\n",
    "# 그렇지 않으면 Kaggle 데이터셋 경로를 사용합니다.\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools  # itertools는 반복 가능한 데이터 구조를 효율적으로 순회하거나 조작하기 위해 사용됩니다.\n",
    "from typing import Iterable # Iterable 타입은 함수나 메서드가 반복 가능한 객체를 인자로 받을 수 있음을 명시하기 위해 사용됩니다.\n",
    "\n",
    "# GemmaFormatter 클래스: 사용자와 모델의 대화를 포매팅합니다.\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>' # 대화 시작을 나타내는 토큰\n",
    "    _end_token = '<end_of_turn>'  # 대화 종료를 나타내는 토큰\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        # 생성자: 시스템 프롬프트와 예시 대화를 초기화합니다.\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 객체를 문자열로 표현할 때 사용됩니다.\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        # 사용자의 대화를 상태에 추가합니다.\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        # 모델의 대화를 상태에 추가합니다.\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        # 사용자의 턴 시작을 상태에 추가합니다.\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        # 모델의 턴 시작을 상태에 추가합니다.\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        # 턴 종료를 상태에 추가합니다.\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        # 상태를 초기화합니다. 이는 새로운 게임이나 대화 세션을 시작할 때 사용됩니다.\n",
    "        self._state = \"\"\n",
    "        # 시스템 프롬프트가 설정되어 있다면, 사용자의 첫 번째 턴으로 추가합니다.\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        # 몇 가지 예시 대화가 제공되었다면, 이를 초기 대화로 설정합니다.\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self # 초기화된 상태를 반환합니다.\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # start_agent에 따라 대화 순서를 결정합니다. 'model'이면 모델부터 시작합니다.\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        # formatters 리스트를 순환하며, 사용자와 모델의 턴을 번갈아 가면서 진행합니다.\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        # turns 리스트에 있는 각 대화(turn)에 대해, 해당하는 formatter 함수를 호출합니다.\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)  # 대화를 현재 상태에 추가합니다.\n",
    "        return self  # 메서드 체이닝을 위해 객체 자신을 반환합니다.\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    \"\"\"\n",
    "    주어진 dtype으로 PyTorch의 기본 텐서 데이터 타입을 임시로 설정하는 컨텍스트 매니저입니다.\n",
    "    이 함수는 with 문 내에서 사용될 때, 지정된 dtype으로 기본 텐서 타입을 설정하고,\n",
    "    with 블록이 종료되면 기본 텐서 타입을 torch.float으로 재설정합니다.\n",
    "    \n",
    "    Args:\n",
    "    dtype (torch.dtype): 설정하고자 하는 PyTorch 텐서의 데이터 타입.\n",
    "    \n",
    "    Yields:\n",
    "    None: 이 컨텍스트 매니저는 특정 값을 생성하지 않습니다.\n",
    "    \"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        # 생성자: 모델 변형, 장치, 시스템 프롬프트, 예시 대화를 초기화합니다.\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        # 모델 설정을 로드합니다. '2b' 변형이면 get_config_for_2b를, 아니면 get_config_for_7b를 사용합니다.\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # 주어진 dtype으로 모델을 초기화합니다.\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        # 관찰(obs)을 기반으로 세션을 시작하고 LLM을 호출하여 응답을 생성합니다.\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        # 세션 시작 메서드: 구현되지 않았습니다.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        # LLM을 호출하여 주어진 프롬프트에 대한 응답을 생성합니다.\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 응답에서 키워드를 파싱합니다.\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 응답을 파싱하는 메서드: 구현되지 않음\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    # 두 리스트 x, y를 받아 서로 다른 길이의 리스트를 교차로 결합합니다.\n",
    "    # itertools.zip_longest를 사용하여 더 긴 리스트의 끝까지 요소를 포함시키고, None이 아닌 요소만 반환합니다.\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # GemmaAgent 클래스를 상속받아 초기화합니다.\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 세션을 시작하는 메서드입니다. 포매터를 리셋하고, 사용자에게 20 Questions 게임의 역할을 알립니다.\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        # 질문과 답변을 교차로 결합하여 대화 순서를 생성합니다.\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        # 사용자의 턴 타입에 따라 다음 단계의 안내를 합니다.\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 모델의 응답을 파싱하여 질문 또는 추측을 반환합니다.\n",
    "        if obs.turnType == 'ask':\n",
    "            # 응답에서 질문을 추출합니다. '*'가 제거된 응답에서 첫 번째 '?'까지를 질문으로 간주합니다.\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            # 응답에서 추측된 키워드를 추출합니다.\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            # 알 수 없는 턴 타입에 대한 예외 처리입니다.\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # GemmaAgent 클래스를 상속받아 초기화합니다.\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 세션을 시작하는 메서드입니다. 포매터를 리셋하고, 답변자 역할과 키워드 정보를 사용자에게 알립니다.\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        # 질문과 답변을 교차로 결합하여 대화 순서를 생성합니다.\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        # 사용자에게 키워드에 대한 예/아니오 답변을 요청하고, 답변을 별표로 감싸도록 안내합니다.\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 모델의 응답에서 키워드를 파싱하여 'yes' 또는 'no'로 응답합니다.\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "# **중요:** 에이전트를 전역 변수로 정의하여 필요한 에이전트만 로드합니다.\n",
    "# 두 에이전트를 모두 로드하면 OOM(Out of Memory)이 발생할 수 있습니다.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # 'questioner' 이름이 주어지고 에이전트가 아직 초기화되지 않았다면, 질문자 에이전트를 생성합니다.\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    # 'answerer' 이름이 주어지고 에이전트가 아직 초기화되지 않았다면, 답변자 에이전트를 생성합니다.\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt, \n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    # 에이전트가 정상적으로 초기화되었는지 확인합니다.\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # 관찰된 턴 타입에 따라 적절한 에이전트를 호출합니다.\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    # 응답이 None이거나 길이가 1 이하인 경우 기본적으로 \"yes\"를 반환합니다.\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ],
   "id": "5a6249fddf2efc5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T13:46:24.253810Z",
     "start_time": "2024-07-20T13:46:24.093753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pigz와 pv 패키지를 설치합니다. 설치 과정에서 발생하는 모든 출력은 /dev/null로 리다이렉트되어 화면에 표시되지 않습니다.\n",
    "\n",
    "!apt install pigz pv > /dev/null"
   ],
   "id": "ba2d5934ecc84ec0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime that supports apt.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pigz와 pv를 사용하여 압축 프로그램을 지정하고, submission.tar.gz 파일을 생성합니다.\n",
    "# -C 옵션으로 작업 디렉토리를 /kaggle/working/submission으로 변경한 후 현재 디렉토리(.)의 모든 파일을 추가합니다.\n",
    "# 이후, -C 옵션으로 작업 디렉토리를 /kaggle/input으로 변경하고, gemma/pytorch/7b-it-quant/2 디렉토리를 추가합니다.\n",
    "# 이 명령은 submission.tar.gz 파일 내에 두 위치의 파일들을 포함시키며, 압축 과정에서 발생하는 모든 출력은 pv를 통해 시각적으로 표시됩니다.\n",
    "\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ],
   "id": "ed1394826753d1ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
