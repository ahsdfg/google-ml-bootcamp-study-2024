{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%bash\nmkdir -p /kaggle/working/submission\nmkdir -p /tmp/model\npip install -q bitsandbytes accelerate\npip install -qU transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-24T07:56:16.561269Z","iopub.execute_input":"2024-07-24T07:56:16.561808Z","iopub.status.idle":"2024-07-24T07:56:55.877232Z","shell.execute_reply.started":"2024-07-24T07:56:16.561779Z","shell.execute_reply":"2024-07-24T07:56:55.876190Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\n\nHF_TOKEN: str | None  = None\n\ntry:\n    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-07-24T07:57:29.119536Z","iopub.execute_input":"2024-07-24T07:57:29.120324Z","iopub.status.idle":"2024-07-24T07:57:29.229199Z","shell.execute_reply.started":"2024-07-24T07:57:29.120290Z","shell.execute_reply":"2024-07-24T07:57:29.228469Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nfrom pathlib import Path\nimport shutil\n\ng_model_path = Path(\"/tmp/model\")\nif g_model_path.exists():\n    shutil.rmtree(g_model_path)\ng_model_path.mkdir(parents=True)\n\nsnapshot_download(\n    repo_id=\"Qwen/Qwen2-7B-Instruct\",\n    ignore_patterns=\"original*\",\n    local_dir=g_model_path,\n    token=globals().get(\"HF_TOKEN\", None)\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T07:57:31.289237Z","iopub.execute_input":"2024-07-24T07:57:31.289937Z","iopub.status.idle":"2024-07-24T08:00:26.855396Z","shell.execute_reply.started":"2024-07-24T07:57:31.289908Z","shell.execute_reply":"2024-07-24T08:00:26.854547Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa11b40978734e588421d3e53ec5ecc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0100d98fa5e64d6582a17466ef4746b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a96bcffc9ea4e509d1b043fb7788257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32020abe4b314b788e3cfe51249b99a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c416c5858044578c0c83a2d9a0510b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b048ba768a14232b2ba11e4321e857b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7b2bc94e3b44e5a0496a9789fc8203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39f3969d0a341b3a3233a78493fb67d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423621b2997341eebc8063b3d9feb543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d78545662141445aaf3d20d47d1d8118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f36182c28164fa798f20e9a9f110675"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeba374d28804f95b96ae9bf6577d488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9eac1ac07e4653898cdb3773507179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664d3d3c6f2947ec96bef60f8c98ff85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0246f675464f028293bb63bc3a7aa3"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/tmp/model'"},"metadata":{}}]},{"cell_type":"code","source":"!ls -l /tmp/model","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:01:02.848478Z","iopub.execute_input":"2024-07-24T08:01:02.848829Z","iopub.status.idle":"2024-07-24T08:01:03.847613Z","shell.execute_reply.started":"2024-07-24T08:01:02.848804Z","shell.execute_reply":"2024-07-24T08:01:03.846667Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"total 14885580\n-rw-r--r-- 1 root root      11344 Jul 24 07:57 LICENSE\n-rw-r--r-- 1 root root       6551 Jul 24 07:57 README.md\n-rw-r--r-- 1 root root        663 Jul 24 07:57 config.json\n-rw-r--r-- 1 root root        243 Jul 24 07:57 generation_config.json\n-rw-r--r-- 1 root root    1671839 Jul 24 07:57 merges.txt\n-rw-r--r-- 1 root root 3945426872 Jul 24 07:58 model-00001-of-00004.safetensors\n-rw-r--r-- 1 root root 3864726352 Jul 24 07:58 model-00002-of-00004.safetensors\n-rw-r--r-- 1 root root 3864726408 Jul 24 08:00 model-00003-of-00004.safetensors\n-rw-r--r-- 1 root root 3556392240 Jul 24 07:58 model-00004-of-00004.safetensors\n-rw-r--r-- 1 root root      27752 Jul 24 07:57 model.safetensors.index.json\n-rw-r--r-- 1 root root    7028015 Jul 24 07:57 tokenizer.json\n-rw-r--r-- 1 root root       1288 Jul 24 07:57 tokenizer_config.json\n-rw-r--r-- 1 root root    2776833 Jul 24 07:57 vocab.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# load model on memory\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\ndownloaded_model = \"/tmp/model\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    downloaded_model,\n    quantization_config = bnb_config,\n    torch_dtype = torch.float16,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(downloaded_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:01:07.203575Z","iopub.execute_input":"2024-07-24T08:01:07.203978Z","iopub.status.idle":"2024-07-24T08:01:37.799589Z","shell.execute_reply.started":"2024-07-24T08:01:07.203943Z","shell.execute_reply":"2024-07-24T08:01:37.798760Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4532ce69c7402c87407c5cebaf1ace"}},"metadata":{}}]},{"cell_type":"code","source":"# save model in submission directory\nmodel.save_pretrained(\"/kaggle/working/submission/model\")\ntokenizer.save_pretrained(\"/kaggle/working/submission/model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:05:41.740856Z","iopub.execute_input":"2024-07-24T08:05:41.741741Z","iopub.status.idle":"2024-07-24T08:05:55.147336Z","shell.execute_reply.started":"2024-07-24T08:05:41.741709Z","shell.execute_reply":"2024-07-24T08:05:55.146332Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/submission/model/tokenizer_config.json',\n '/kaggle/working/submission/model/special_tokens_map.json',\n '/kaggle/working/submission/model/vocab.json',\n '/kaggle/working/submission/model/merges.txt',\n '/kaggle/working/submission/model/added_tokens.json',\n '/kaggle/working/submission/model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# unload model from memory\nimport gc, torch\ndel model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:06:48.607951Z","iopub.execute_input":"2024-07-24T08:06:48.608898Z","iopub.status.idle":"2024-07-24T08:06:49.025213Z","shell.execute_reply.started":"2024-07-24T08:06:48.608861Z","shell.execute_reply":"2024-07-24T08:06:49.024286Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Qwen-2 모델의 토크나이저 로드\ntokenizer = AutoTokenizer.from_pretrained(downloaded_model)\n\n# End token을 확인\nend_token = tokenizer.bos_token\nend_token_id = tokenizer.eos_token_id\n\nprint(f\"End token: {end_token}\")\nprint(f\"End token ID: {end_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:14:48.409298Z","iopub.execute_input":"2024-07-24T08:14:48.409670Z","iopub.status.idle":"2024-07-24T08:14:48.652816Z","shell.execute_reply.started":"2024-07-24T08:14:48.409644Z","shell.execute_reply":"2024-07-24T08:14:48.651896Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"End token: None\nEnd token ID: 151645\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile submission/main.py\n# comment magic command before simulation\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nimport sys\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\n\n \nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\nelse:\n    MODEL_PATH = \"/kaggle/working/submission/model\"\n\n    \nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nid_eot = tokenizer.convert_tokens_to_ids([\"<|im_end|>\"])[0]\n\n\ndef generate_answer(template):\n    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n    start_gen = inp_ids.input_ids.shape[1]\n    out_ids = out_ids[start_gen:]\n    if id_eot in out_ids:\n        stop = out_ids.tolist().index(id_eot)\n        out = tokenizer.decode(out_ids[:stop])\n    else:\n        out = tokenizer.decode(out_ids)\n    return out\n    \n\nclass Robot:\n    def __init__(self):\n        pass\n    \n    def on(self, mode, obs):\n        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n        if mode == \"asking\":\n            #launch the asker role\n            output = self.asker(obs)\n        if mode == \"answering\":\n            #launch the answerer role\n            output = self.answerer(obs)\n            if \"yes\" in output.lower():\n                output = \"yes\"\n            elif \"no\" in output.lower():\n                output = \"no\"   \n            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n                output = \"yes\"\n        if mode == \"guessing\":\n            #launch the guesser role\n            output = self.asker(obs)\n        return output\n    \n    \n    def asker(self, obs):\n        sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n        the user is going to think of a word, it can be only one of the following 2 categories:\n        1. a place\n        2. a thing\n        So focus your area of search on these options. and give smart questions that narrows down the search space\\n\"\"\"\n    \n        if obs.turnType ==\"ask\":\n            ask_prompt = sys_prompt + \"\"\"your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n            to help you, here's an example of how it should work assuming that the keyword is Morocco:\n            examle:\n            <assistant: is it a place?\n            user: yes\n            assistant: is it in europe?\n            user: no\n            assistant: is it in africa?\n            user: yes\n            assistant: do most people living there have dark skin?\n            user: no\n            assistant: is it a country name starting by m ?\n            user: yes\n            assistant: is it Morocco?\n            user: yes>\n\n            the user has chosen the word, ask your first question!\n            please be short and not verbose, give only one question, no extra word!\"\"\"\n            chat_template = f\"\"\"<s><|system|> \\n\\n{ask_prompt}<|im_end|>\"\"\"\n            chat_template += \"<|assistant|> \\n\\n\"\n            if len(obs.questions)>=1:\n                for q, a in zip(obs.questions, obs.answers):\n                    chat_template += f\"{q}<|im_end|><|user|> \\n\\n\"\n                    chat_template += f\"{a}<|im_end|><|assistant|> \\n\\n\"\n                    \n        elif obs.turnType == \"guess\":\n            conv = \"\"\n            for q, a in zip(obs.questions, obs.answers):\n                conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n            guess_prompt =  sys_prompt + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n            based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n            chat_template = f\"\"\"<s><|system|> \\n\\n{guess_prompt}<|im_end|>\"\"\"\n            chat_template += \"<|assistant|> \\n\\n\"\n                \n        output = generate_answer(chat_template)        \n        return output\n        \n        \n        \n    def answerer(self, obs):\n        sys_prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n        the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n        Know that the user will always guess a word belonging to one of the following 2 categories:\n        1. a place\n        2. a thing\n        so make sure you understand the user's question and you understand the keyword you're playig on.\n        for now the word that the user should guess is: \"{obs.keyword}\", it is of category \"{obs.category}\",\n        to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n        examle:\n        <user: is it a place?\n        assistant: yes\n        user: is it in europe?\n        assistant: no\n        user: is it in africa?\n        assistant: yes\n        user: do most people living there have dark skin?\n        assistant: no\n        user: is it a country name starting by m ?\n        assistant: yes\n        user: is it Morocco?\n        assistant: yes>\"\"\"\n        \n        chat_template = f\"\"\"<s><|system|> \\n\\n{sys_prompt}<|im_end|>\"\"\"\n        chat_template += \"<|user|> \\n\\n\"\n        chat_template += f\"{obs.questions[0]}<|im_end|>\"\n        chat_template += \"<|assistant|> \\n\\n\"\n        if len(obs.answers)>=1:\n            for q, a in zip(obs.questions[1:], obs.answers):\n                chat_template += f\"{a}<|im_end|><|user|> \\n\\n\"\n                chat_template += f\"{q}<|im_end|><|assistant|> \\n\\n\"\n        output = generate_answer(chat_template)\n        return output\n    \n    \nrobot = Robot()\n\n\ndef agent(obs, cfg):\n    \n    if obs.turnType ==\"ask\":\n        response = robot.on(mode = \"asking\", obs = obs)\n        \n    elif obs.turnType ==\"guess\":\n        response = robot.on(mode = \"guessing\", obs = obs)\n        \n    elif obs.turnType ==\"answer\":\n        response = robot.on(mode = \"answering\", obs = obs)\n        \n    if response == None or len(response)<=1:\n        response = \"yes\"\n        \n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:06:58.456472Z","iopub.execute_input":"2024-07-24T08:06:58.456836Z","iopub.status.idle":"2024-07-24T08:06:58.467317Z","shell.execute_reply.started":"2024-07-24T08:06:58.456806Z","shell.execute_reply":"2024-07-24T08:06:58.466336Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Overwriting submission/main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!apt install pigz pv > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:07:01.388304Z","iopub.execute_input":"2024-07-24T08:07:01.388705Z","iopub.status.idle":"2024-07-24T08:07:04.104431Z","shell.execute_reply.started":"2024-07-24T08:07:01.388671Z","shell.execute_reply":"2024-07-24T08:07:04.103273Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T08:07:05.750083Z","iopub.execute_input":"2024-07-24T08:07:05.750469Z","iopub.status.idle":"2024-07-24T08:08:38.690913Z","shell.execute_reply.started":"2024-07-24T08:07:05.750438Z","shell.execute_reply":"2024-07-24T08:08:38.689780Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"4.56GiB 0:01:31 [51.2MiB/s] [       <=>                                        ]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}