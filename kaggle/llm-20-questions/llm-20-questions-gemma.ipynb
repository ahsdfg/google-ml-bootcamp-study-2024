{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%bash\ncd /kaggle/working\npip install -qU transformers bitsandbytes accelerate\nmkdir -p /kaggle/working/submission/lib/gemma/\n# mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-20T06:19:56.936140Z","iopub.execute_input":"2024-07-20T06:19:56.936515Z","iopub.status.idle":"2024-07-20T06:20:32.742164Z","shell.execute_reply.started":"2024-07-20T06:19:56.936482Z","shell.execute_reply":"2024-07-20T06:20:32.741284Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\nfrom kaggle_secrets import UserSecretsClient\n\nhuggingface_hub.login(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T06:20:32.744167Z","iopub.execute_input":"2024-07-20T06:20:32.745183Z","iopub.status.idle":"2024-07-20T06:20:33.553774Z","shell.execute_reply.started":"2024-07-20T06:20:32.745142Z","shell.execute_reply":"2024-07-20T06:20:33.552755Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nmodel_id = \"google/gemma-2-9b-it\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = bnb_config,\n    torch_dtype = torch.float16,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T06:20:33.554993Z","iopub.execute_input":"2024-07-20T06:20:33.555298Z","iopub.status.idle":"2024-07-20T06:23:42.502737Z","shell.execute_reply.started":"2024-07-20T06:20:33.555270Z","shell.execute_reply":"2024-07-20T06:23:42.501854Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04adccd0068b40b4af19aacf51176af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f991574f0c4e9085d160c41ecb0922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9421aaa55cf74be19f1df8e3e05eb6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2393ad921f48389184cc79ee2dfe77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9a87f2f28d4a7b8da7e69e86a81d84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af31b20ef5f84d9ab120453dc671e7aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36d4bc636944d48b96afa0bb1462d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cb9dbf465840f6b3e8d07f2f6eeab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ff56a92704491cbae97cba554990b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2487e2e417647fe8dab8a6091f70c4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97504f2e09d644089e8e7b96bbf661cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f664ec4805fb4bb4a880985925911e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e318c5a8308499bac4f64d327132cad"}},"metadata":{}}]},{"cell_type":"code","source":"import os, sys\n\n# sys.path.insert(0, \"/kaggle/working/submission/lib/gemma\")\n# sys.path.append(\"/kaggle/input/llm-20-questions/llm_20_questions\")\n\nmodel.save_pretrained(\"/kaggle/working/submission/lib/gemma\")\ntokenizer.save_pretrained(\"/kaggle/working/submission/lib/gemma\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T06:23:42.504686Z","iopub.execute_input":"2024-07-20T06:23:42.505145Z","iopub.status.idle":"2024-07-20T06:24:00.157371Z","shell.execute_reply.started":"2024-07-20T06:23:42.505115Z","shell.execute_reply":"2024-07-20T06:24:00.156385Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/submission/lib/gemma/tokenizer_config.json',\n '/kaggle/working/submission/lib/gemma/special_tokens_map.json',\n '/kaggle/working/submission/lib/gemma/tokenizer.model',\n '/kaggle/working/submission/lib/gemma/added_tokens.json',\n '/kaggle/working/submission/lib/gemma/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import gc, torch\ndel model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:09:08.864617Z","iopub.execute_input":"2024-07-20T05:09:08.865132Z","iopub.status.idle":"2024-07-20T05:09:09.307470Z","shell.execute_reply.started":"2024-07-20T05:09:08.865098Z","shell.execute_reply":"2024-07-20T05:09:09.306678Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%writefile submission/main.py\n# Setup\nimport os\nimport sys\n\n# **IMPORTANT:** Set up your system path like this to make your code work\n# both in notebooks and in the simulations environment.\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\nelse:\n    sys.path.insert(0, \"/kaggle/working/submission/lib/gemma\")\n\nimport contextlib\nimport os\nimport sys\nfrom pathlib import Path\n\n# import torch\n# from gemma.config import GemmaConfig, get_model_config\n# from gemma.model import GemmaForCausalLM\n# from gemma.tokenizer import Tokenizer\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif os.path.exists(KAGGLE_AGENT_PATH):\n    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma\")\nelse:\n    MODEL_PATH = \"/kaggle/working/submission/lib/gemma\"\n\n# Prompt Formatting\nimport itertools\nfrom typing import Iterable\n\n\nclass GemmaFormatter:\n    _start_token = '<start_of_turn>'\n    _end_token = '<end_of_turn>'\n\n    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n        self._system_prompt = system_prompt\n        self._few_shot_examples = few_shot_examples\n        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n        self.reset()\n\n    def __repr__(self):\n        return self._state\n\n    def user(self, prompt):\n        self._state += self._turn_user.format(prompt)\n        return self\n\n    def model(self, prompt):\n        self._state += self._turn_model.format(prompt)\n        return self\n\n    def start_user_turn(self):\n        self._state += f\"{self._start_token}user\\n\"\n        return self\n\n    def start_model_turn(self):\n        self._state += f\"{self._start_token}model\\n\"\n        return self\n\n    def end_turn(self):\n        self._state += f\"{self._end_token}\\n\"\n        return self\n\n    def reset(self):\n        self._state = \"\"\n        if self._system_prompt is not None:\n            self.user(self._system_prompt)\n        if self._few_shot_examples is not None:\n            self.apply_turns(self._few_shot_examples, start_agent='user')\n        return self\n\n    def apply_turns(self, turns: Iterable, start_agent: str):\n        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n        formatters = itertools.cycle(formatters)\n        for fmt, turn in zip(formatters, turns):\n            fmt(turn)\n        return self\n\n\n# Agent Definitions\nimport re\n\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n\nclass GemmaAgent:\n    def __init__(self, model_path=MODEL_PATH, device='cuda:0', system_prompt=None, few_shot_examples=None):\n        self._device = torch.device(device)\n        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n\n        print(\"Initializing model\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quanty_type = \"fp4\", \n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quanty = True,\n        )\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            quantization_config = bnb_config,\n            torch_dtype = torch.float16,\n            device_map = \"auto\",\n            trust_remote_code = True,\n        )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n#         model_config = get_model_config(variant)\n#         model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n#         model_config.quant = \"quant\" in variant\n\n#         with _set_default_tensor_type(model_config.get_dtype()):\n#             model = GemmaForCausalLM(model_config)\n#             ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n#             model.load_weights(ckpt_path)\n#             self.model = model.to(self._device).eval()\n\n    def __call__(self, obs, *args):\n        self._start_session(obs)\n        prompt = str(self.formatter)\n        response = self._call_llm(prompt)\n        response = self._parse_response(response, obs)\n        print(f\"{response=}\")\n        return response\n\n    def _start_session(self, obs: dict):\n        raise NotImplementedError\n\n    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n        if sampler_kwargs is None:\n            sampler_kwargs = {\n                'temperature': 0.01,\n                'top_p': 0.1,\n                'top_k': 1,\n        }\n        \n        outputs = self.model.generate(input_ids=inputs.to(model.device), max_new_tokens=max_new_tokens)\n        response = self.model.generate(\n            prompt,\n            device=self._device,\n            output_len=max_new_tokens,\n            **sampler_kwargs,\n        )\n        return response\n\n    def _parse_keyword(self, response: str):\n        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n        if match is None:\n            keyword = ''\n        else:\n            keyword = match.group().lower()\n        return keyword\n\n    def _parse_response(self, response: str, obs: dict):\n        raise NotImplementedError\n\n\ndef interleave_unequal(x, y):\n    return [\n        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n    ]\n\n\nclass GemmaQuestionerAgent(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _start_session(self, obs):\n        self.formatter.reset()\n        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n        turns = interleave_unequal(obs.questions, obs.answers)\n        self.formatter.apply_turns(turns, start_agent='model')\n        if obs.turnType == 'ask':\n            self.formatter.user(\"Please ask a yes-or-no question.\")\n        elif obs.turnType == 'guess':\n            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n        self.formatter.start_model_turn()\n\n    def _parse_response(self, response: str, obs: dict):\n        if obs.turnType == 'ask':\n            match = re.search(\".+?\\?\", response.replace('*', ''))\n            if match is None:\n                question = \"Is it a place?\"\n            else:\n                question = match.group()\n            return question\n        elif obs.turnType == 'guess':\n            guess = self._parse_keyword(response)\n            return guess\n        else:\n            raise ValueError(\"Unknown turn type:\", obs.turnType)\n\n\nclass GemmaAnswererAgent(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _start_session(self, obs):\n        self.formatter.reset()\n        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n        turns = interleave_unequal(obs.questions, obs.answers)\n        self.formatter.apply_turns(turns, start_agent='user')\n        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n        self.formatter.start_model_turn()\n\n    def _parse_response(self, response: str, obs: dict):\n        answer = self._parse_keyword(response)\n        return 'yes' if 'yes' in answer else 'no'\n\n\n# Agent Creation\nsystem_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n\nfew_shot_examples = [\n    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n    \"Is it a thing?\", \"**no**\",\n    \"Is is a place?\", \"**yes**\",\n    \"Is it a country?\", \"**yes**\",\n    \"Does it start with f?\", \"**yes** Now guess the keyword.\",\n    \"**France**\", \"Correct!\",\n]\n\n\n# **IMPORTANT:** Define agent as a global so you only have to load\n# the agent you need. Loading both will likely lead to OOM.\nagent = None\n\n\ndef get_agent(name: str):\n    global agent\n    \n    if agent is None and name == 'questioner':\n        agent = GemmaQuestionerAgent(\n            device='cuda:0',\n            system_prompt=system_prompt,\n            few_shot_examples=few_shot_examples,\n        )\n    elif agent is None and name == 'answerer':\n        agent = GemmaAnswererAgent(\n            device='cuda:0',\n            system_prompt=system_prompt,\n            few_shot_examples=few_shot_examples,\n        )\n    assert agent is not None, \"Agent not initialized.\"\n\n    return agent\n\n\ndef agent_fn(obs, cfg):\n    if obs.turnType == \"ask\":\n        response = get_agent('questioner')(obs)\n    elif obs.turnType == \"guess\":\n        response = get_agent('questioner')(obs)\n    elif obs.turnType == \"answer\":\n        response = get_agent('answerer')(obs)\n    if response is None or len(response) <= 1:\n        return \"yes\"\n    else:\n        return response","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:09:09.309266Z","iopub.execute_input":"2024-07-20T05:09:09.309674Z","iopub.status.idle":"2024-07-20T05:09:10.156821Z","shell.execute_reply.started":"2024-07-20T05:09:09.309640Z","shell.execute_reply":"2024-07-20T05:09:10.155884Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing submission/main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install pigz pv > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ","metadata":{},"execution_count":null,"outputs":[]}]}