{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Prerequisites\nInstall HuggingFace packages and create submission directory.","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/submission","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:11:31.796703Z","iopub.execute_input":"2024-07-21T04:11:31.797132Z","iopub.status.idle":"2024-07-21T04:11:32.873434Z","shell.execute_reply.started":"2024-07-21T04:11:31.797086Z","shell.execute_reply":"2024-07-21T04:11:32.872315Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%time\nimport os,sys\nos.system(\"pip install -q -U -t /kaggle/working/submission/lib accelerate transformers bitsandbytes\")\nos.system(\"pip cache purge\")\nsys.path.insert(0, \"/kaggle/working/submission/lib\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:11:33.833410Z","iopub.execute_input":"2024-07-21T04:11:33.834092Z","iopub.status.idle":"2024-07-21T04:14:26.929371Z","shell.execute_reply.started":"2024-07-21T04:11:33.834053Z","shell.execute_reply":"2024-07-21T04:14:26.928136Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nucx-py 0.38.0 requires libucx<1.16,>=1.15.0, which is not installed.\nucxx 0.38.0 requires libucx>=1.15.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndatasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2024.6.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nrmm 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2024.6.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\ntensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stdout","text":"Files removed: 242\nCPU times: user 11.7 ms, sys: 4.73 ms, total: 16.5 ms\nWall time: 2min 53s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### HuggingFace Login\n\nAdd HugginFace access token to secrets. You can find it in `Add-ons -> secrets`","metadata":{}},{"cell_type":"code","source":"import huggingface_hub\nfrom kaggle_secrets import UserSecretsClient\n\nhuggingface_hub.login(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:14:26.931413Z","iopub.execute_input":"2024-07-21T04:14:26.932155Z","iopub.status.idle":"2024-07-21T04:14:27.609923Z","shell.execute_reply.started":"2024-07-21T04:14:26.932117Z","shell.execute_reply":"2024-07-21T04:14:27.609033Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Download Model via HuggingFace\nIn this notebook, we are using gemma-2-9b model with 4-bit quantization.","metadata":{}},{"cell_type":"code","source":"# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nmodel_id = \"google/gemma-2-9b-it\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = bnb_config,\n    torch_dtype = torch.float16,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:14:27.611333Z","iopub.execute_input":"2024-07-21T04:14:27.611752Z","iopub.status.idle":"2024-07-21T04:17:42.100933Z","shell.execute_reply.started":"2024-07-21T04:14:27.611706Z","shell.execute_reply":"2024-07-21T04:17:42.100031Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26507a6f7a9148d2b57ed668f80876b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c6eb143a1343a9a8d0acd92967e29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f1110d678d4b528f9cdf8d9b2eea08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbb31fd2d41400c9cb9b12303a0145f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c097e39fbe4969a7223438cf7c7d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc2ad6071224b189dd8501e5638bf8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4718a644154d679064de0a21fa5469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb9cc41d2834747baedcfe3b1d5981b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cdf6b9d35aa4954837631880fc9e079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d508428df4c54923b1aca7f652bccd2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c324f70a9d2418aa5d66e0b15e67233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965b899e82dc418e87c1528fb5f1b540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087a2e98d4554f41bb1ae977064da25a"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Save Model\nSave the loaded model and tokenizer in the submission directory.\nRemove the model and tokenizer from the memory.","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/submission/model\")\ntokenizer.save_pretrained(\"/kaggle/working/submission/model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:17:42.103067Z","iopub.execute_input":"2024-07-21T04:17:42.103565Z","iopub.status.idle":"2024-07-21T04:17:58.646486Z","shell.execute_reply.started":"2024-07-21T04:17:42.103537Z","shell.execute_reply":"2024-07-21T04:17:58.645502Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/submission/model/tokenizer_config.json',\n '/kaggle/working/submission/model/special_tokens_map.json',\n '/kaggle/working/submission/model/tokenizer.model',\n '/kaggle/working/submission/model/added_tokens.json',\n '/kaggle/working/submission/model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import gc, torch\ndel model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:17:58.647922Z","iopub.execute_input":"2024-07-21T04:17:58.648565Z","iopub.status.idle":"2024-07-21T04:17:59.115008Z","shell.execute_reply.started":"2024-07-21T04:17:58.648527Z","shell.execute_reply":"2024-07-21T04:17:59.114023Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Agent","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/submission/main.py\n# Setup\nimport os\nimport sys\n\n# **IMPORTANT:** Set up your system path like this to make your code work\n# both in notebooks and in the simulations environment.\n\n\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\nelse:\n    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n\nimport contextlib\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif os.path.exists(KAGGLE_AGENT_PATH):\n    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\nelse:\n    MODEL_PATH = \"/kaggle/working/submission/model\"\n\n# Prompt Formatting\nimport itertools\nfrom typing import Iterable\n\n\nclass GemmaFormatter:\n    _start_token = '<start_of_turn>'\n    _end_token = '<end_of_turn>'\n\n    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n        self._system_prompt = system_prompt\n        self._few_shot_examples = few_shot_examples\n        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n        self.reset()\n\n    def __repr__(self):\n        return self._state\n\n    def user(self, prompt):\n        self._state += self._turn_user.format(prompt)\n        return self\n\n    def model(self, prompt):\n        self._state += self._turn_model.format(prompt)\n        return self\n\n    def start_user_turn(self):\n        self._state += f\"{self._start_token}user\\n\"\n        return self\n\n    def start_model_turn(self):\n        self._state += f\"{self._start_token}model\\n\"\n        return self\n\n    def end_turn(self):\n        self._state += f\"{self._end_token}\\n\"\n        return self\n\n    def reset(self):\n        self._state = \"\"\n        if self._system_prompt is not None:\n            self.user(self._system_prompt)\n        if self._few_shot_examples is not None:\n            self.apply_turns(self._few_shot_examples, start_agent='user')\n        return self\n\n    def apply_turns(self, turns: Iterable, start_agent: str):\n        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n        formatters = itertools.cycle(formatters)\n        for fmt, turn in zip(formatters, turns):\n            fmt(turn)\n        return self\n\n\n# Agent Definitions\nimport re\n\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n\nclass GemmaAgent:\n    def __init__(self, model_path=MODEL_PATH, device='cuda:0', system_prompt=None, few_shot_examples=None):\n        self._device = torch.device(device)\n        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n\n        print(\"Initializing model\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map = \"auto\",\n        )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    def __call__(self, obs, *args):\n        self._start_session(obs)\n        prompt = str(self.formatter)\n        response = self._call_llm(prompt)\n        response = self._parse_response(response, obs)\n        print(f\"{response=}\")\n        return response\n\n    def _start_session(self, obs: dict):\n        raise NotImplementedError\n\n    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n        if sampler_kwargs is None:\n            sampler_kwargs = {\n                'temperature': 0.01,\n                'top_p': 0.1,\n                'top_k': 1,\n        }\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, kwargs=sampler_kwargs)\n        return self.tokenizer.decode(outputs[0])\n\n    def _parse_keyword(self, response: str):\n        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n        if match is None:\n            keyword = ''\n        else:\n            keyword = match.group().lower()\n        return keyword\n\n    def _parse_response(self, response: str, obs: dict):\n        raise NotImplementedError\n\n\ndef interleave_unequal(x, y):\n    return [\n        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n    ]\n\n\nclass GemmaQuestionerAgent(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _start_session(self, obs):\n        self.formatter.reset()\n        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n        turns = interleave_unequal(obs.questions, obs.answers)\n        self.formatter.apply_turns(turns, start_agent='model')\n        if obs.turnType == 'ask':\n            self.formatter.user(\"Please ask a yes-or-no question.\")\n        elif obs.turnType == 'guess':\n            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n        self.formatter.start_model_turn()\n\n    def _parse_response(self, response: str, obs: dict):\n        if obs.turnType == 'ask':\n            match = re.search(\".+?\\?\", response.replace('*', ''))\n            if match is None:\n                question = \"Is it a place?\"\n            else:\n                question = match.group()\n            return question\n        elif obs.turnType == 'guess':\n            guess = self._parse_keyword(response)\n            return guess\n        else:\n            raise ValueError(\"Unknown turn type:\", obs.turnType)\n\n\nclass GemmaAnswererAgent(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _start_session(self, obs):\n        self.formatter.reset()\n        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n        turns = interleave_unequal(obs.questions, obs.answers)\n        self.formatter.apply_turns(turns, start_agent='user')\n        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n        self.formatter.start_model_turn()\n\n    def _parse_response(self, response: str, obs: dict):\n        answer = self._parse_keyword(response)\n        return 'yes' if 'yes' in answer else 'no'\n\n\n# Agent Creation\nsystem_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n\nfew_shot_examples = [\n    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n    \"Is it a thing?\", \"**no**\",\n    \"Is is a place?\", \"**yes**\",\n    \"Is it a country?\", \"**yes**\",\n    \"Does it start with f?\", \"**yes** Now guess the keyword.\",\n    \"**France**\", \"Correct!\",\n]\n\n\n# **IMPORTANT:** Define agent as a global so you only have to load\n# the agent you need. Loading both will likely lead to OOM.\nagent = None\n\n\ndef get_agent(name: str):\n    global agent\n    \n    if agent is None and name == 'questioner':\n        agent = GemmaQuestionerAgent(\n            device='cuda:0',\n            system_prompt=system_prompt,\n            few_shot_examples=few_shot_examples,\n        )\n    elif agent is None and name == 'answerer':\n        agent = GemmaAnswererAgent(\n            device='cuda:0',\n            system_prompt=system_prompt,\n            few_shot_examples=few_shot_examples,\n        )\n    assert agent is not None, \"Agent not initialized.\"\n\n    return agent\n\n\ndef agent_fn(obs, cfg):\n    if obs.turnType == \"ask\":\n        response = get_agent('questioner')(obs)\n    elif obs.turnType == \"guess\":\n        response = get_agent('questioner')(obs)\n    elif obs.turnType == \"answer\":\n        response = get_agent('answerer')(obs)\n    if response is None or len(response) <= 1:\n        return \"yes\"\n    else:\n        return response","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:18:49.984707Z","iopub.execute_input":"2024-07-21T04:18:49.985110Z","iopub.status.idle":"2024-07-21T04:18:49.997141Z","shell.execute_reply.started":"2024-07-21T04:18:49.985071Z","shell.execute_reply":"2024-07-21T04:18:49.996228Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/submission/main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install pigz pv > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:19:38.764944Z","iopub.execute_input":"2024-07-21T04:19:38.765363Z","iopub.status.idle":"2024-07-21T04:19:45.596700Z","shell.execute_reply.started":"2024-07-21T04:19:38.765334Z","shell.execute_reply":"2024-07-21T04:19:45.595225Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast --recursive | pv' -cf /kaggle/working/submission.tar.gz -C /kaggle/working/submission .","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:20:12.765633Z","iopub.execute_input":"2024-07-21T04:20:12.767060Z","iopub.status.idle":"2024-07-21T04:23:30.681848Z","shell.execute_reply.started":"2024-07-21T04:20:12.766994Z","shell.execute_reply":"2024-07-21T04:23:30.680731Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"8.11GiB 0:03:16 [42.2MiB/s] [  <=>                                             ]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Simulate Game","metadata":{}},{"cell_type":"markdown","source":"### Load test data\nDownload the latest keywords.py from [kaggle-environments](https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/llm_20_questions/keywords.py) github repo","metadata":{}},{"cell_type":"code","source":"%%bash\n\nwget -O keywords.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\nmkdir -p /kaggle/working/simulation/","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:05.556195Z","iopub.execute_input":"2024-07-21T04:25:05.556633Z","iopub.status.idle":"2024-07-21T04:25:05.809476Z","shell.execute_reply.started":"2024-07-21T04:25:05.556599Z","shell.execute_reply":"2024-07-21T04:25:05.808516Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"--2024-07-21 04:25:05--  https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93552 (91K) [text/plain]\nSaving to: 'keywords.py'\n\n     0K .......... .......... .......... .......... .......... 54% 3.00M 0s\n    50K .......... .......... .......... .......... .         100% 27.6M=0.02s\n\n2024-07-21 04:25:05 (5.03 MB/s) - 'keywords.py' saved [93552/93552]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom keywords import KEYWORDS_JSON\n\ndef create_keyword_df(KEYWORDS_JSON):\n    json_data = json.loads(KEYWORDS_JSON)\n\n    keyword_list = []\n    category_list = []\n    alts_list = []\n\n    for i in range(len(json_data)):\n        for j in range(len(json_data[i]['words'])):\n            keyword = json_data[i]['words'][j]['keyword']\n            keyword_list.append(keyword)\n            category_list.append(json_data[i]['category'])\n            alts_list.append(json_data[i]['words'][j]['alts'])\n\n    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n    data_pd['keyword'] = keyword_list\n    data_pd['category'] = category_list\n    data_pd['alts'] = alts_list\n    \n    return data_pd\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:17.925113Z","iopub.execute_input":"2024-07-21T04:25:17.925811Z","iopub.status.idle":"2024-07-21T04:25:18.525138Z","shell.execute_reply.started":"2024-07-21T04:25:17.925778Z","shell.execute_reply":"2024-07-21T04:25:18.524304Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"keywords = create_keyword_df(KEYWORDS_JSON)\n# keywords_df.head(5)\nkeywords.tail(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:18.527087Z","iopub.execute_input":"2024-07-21T04:25:18.528215Z","iopub.status.idle":"2024-07-21T04:25:18.566600Z","shell.execute_reply.started":"2024-07-21T04:25:18.528178Z","shell.execute_reply":"2024-07-21T04:25:18.565560Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"            keyword category                   alts\n1137          rhine    place                     []\n1138  yangtze river    place  [changjiang, yangtze]\n1139   yellow river    place             [huang he]\n1140  zambezi river    place              [zambezi]\n1141  yenisei river    place              [yenisei]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>category</th>\n      <th>alts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1137</th>\n      <td>rhine</td>\n      <td>place</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>yangtze river</td>\n      <td>place</td>\n      <td>[changjiang, yangtze]</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>yellow river</td>\n      <td>place</td>\n      <td>[huang he]</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>zambezi river</td>\n      <td>place</td>\n      <td>[zambezi]</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>yenisei river</td>\n      <td>place</td>\n      <td>[yenisei]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"keywords[\"category\"].unique()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:18.567685Z","iopub.execute_input":"2024-07-21T04:25:18.567941Z","iopub.status.idle":"2024-07-21T04:25:18.577195Z","shell.execute_reply.started":"2024-07-21T04:25:18.567918Z","shell.execute_reply":"2024-07-21T04:25:18.576232Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array(['things', 'place'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"keywords.to_csv(\"/kaggle/working/simulation/keywords.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:19.824742Z","iopub.execute_input":"2024-07-21T04:25:19.825161Z","iopub.status.idle":"2024-07-21T04:25:19.837550Z","shell.execute_reply.started":"2024-07-21T04:25:19.825133Z","shell.execute_reply":"2024-07-21T04:25:19.836657Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Create Agents\n2 vs 2","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/simulation/agent1.py\n\nimport pandas as pd\nimport numpy as np\n\nkeywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n\ndef agent_fn(obs, cfg):\n    global keywords\n    \n    # DISPLAY ROUND NUMBER\n    k = len( obs.questions )\n    if obs.turnType == \"ask\":\n        print()\n        print(\"#\"*25)\n        print(f\"### Round {k+1}\")\n        print(\"#\"*25)\n\n    # DISPLAY AGENT NAME AND JSON INPUT\n    name = \"Team 1 - Questioner - Agent Random\"\n    print(f\"\\n{name}\\nINPUT =\",obs)\n    \n    # GENERATE RESPONSE\n    keyword = np.random.choice(keywords)\n    if obs.turnType == \"ask\":\n        response = f\"Is it {keyword}?\"\n    else: #obs.turnType == \"guess\"\n        response = keyword\n        if obs.answers[-1] == \"yes\":\n            response = obs.questions[-1].rsplit(\" \",1)[1][:-1]\n    print(f\"OUTPUT = '{response}'\")\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:31.473042Z","iopub.execute_input":"2024-07-21T04:25:31.473458Z","iopub.status.idle":"2024-07-21T04:25:31.480425Z","shell.execute_reply.started":"2024-07-21T04:25:31.473413Z","shell.execute_reply":"2024-07-21T04:25:31.479371Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/simulation/agent1.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/simulation/agent2.py\n\nimport numpy as np\n\ndef agent_fn(obs, cfg):\n    \n    # DISPLAY AGENT NAME AND JSON INPUT\n    name = \"Team 1 - Answerer - Agent Random\"\n    print(f\"\\n{name}\\nINPUT =\",obs)\n    \n    # GENERATE RESPONSE\n    response = \"no\"\n    #response = np.random.choice([\"yes\",\"no\"])\n    if obs.keyword.lower() in obs.questions[-1].lower():\n        response = \"yes\"\n    print(f\"OUTPUT = '{response}'\")\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:31.617752Z","iopub.execute_input":"2024-07-21T04:25:31.618080Z","iopub.status.idle":"2024-07-21T04:25:31.623950Z","shell.execute_reply.started":"2024-07-21T04:25:31.618036Z","shell.execute_reply":"2024-07-21T04:25:31.622820Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/simulation/agent2.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/simulation/agent3.py\n\nimport pandas as pd\nimport numpy as np\n\nkeywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n\ndef agent_fn(obs, cfg):\n    global keywords\n    \n    # DISPLAY AGENT NAME AND JSON INPUT\n    name = \"Team 2 - Questioner - Agent Random\"\n    print(f\"\\n{name}\\nINPUT =\",obs)\n    \n    # GENERATE RESPONSE\n    keyword = np.random.choice(keywords)\n    if obs.turnType == \"ask\":\n        response = f\"Is it {keyword}?\"\n    else: #obs.turnType == \"guess\"\n        response = keyword\n        if obs.answers[-1] == \"yes\":\n            response = obs.questions[-1].rsplit(\" \",1)[1][:-1]\n    print(f\"OUTPUT = '{response}'\")\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:31.792211Z","iopub.execute_input":"2024-07-21T04:25:31.792523Z","iopub.status.idle":"2024-07-21T04:25:31.799174Z","shell.execute_reply.started":"2024-07-21T04:25:31.792494Z","shell.execute_reply":"2024-07-21T04:25:31.798164Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/simulation/agent3.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/simulation/agent4.py\n\nimport numpy as np\n\ndef agent_fn(obs, cfg):\n    \n    # DISPLAY AGENT NAME AND JSON INPUT\n    name = \"Team 2 - Answerer - Agent Random\"\n    print(f\"\\n{name}\\nINPUT =\",obs)\n    \n    # GENERATE RESPONSE\n    response = \"no\"\n    #response = np.random.choice([\"yes\",\"no\"])\n    if obs.keyword.lower() in obs.questions[-1].lower():\n        response = \"yes\"\n    print(f\"OUTPUT = '{response}'\")\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:32.345032Z","iopub.execute_input":"2024-07-21T04:25:32.345411Z","iopub.status.idle":"2024-07-21T04:25:32.351506Z","shell.execute_reply.started":"2024-07-21T04:25:32.345385Z","shell.execute_reply":"2024-07-21T04:25:32.350492Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/simulation/agent4.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Create Environment","metadata":{}},{"cell_type":"code","source":"!pip install -q pygame","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:35.071770Z","iopub.execute_input":"2024-07-21T04:25:35.072538Z","iopub.status.idle":"2024-07-21T04:25:51.017306Z","shell.execute_reply.started":"2024-07-21T04:25:35.072500Z","shell.execute_reply":"2024-07-21T04:25:51.015868Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"GEMMA_AS_QUESTIONER = True\nGEMMA_AS_ANSWERER = True\n\nfrom kaggle_environments import make\nenv = make(\"llm_20_questions\", debug=True)\n\n# TEAM 1\nagent1 = \"/kaggle/working/simulation/agent1.py\"\nagent2 = \"/kaggle/working/simulation/agent2.py\"\n\n# TEAM 2 - QUESTIONER\nagent3 = \"/kaggle/working/simulation/agent3.py\"\nif GEMMA_AS_QUESTIONER:\n    agent3 = \"/kaggle/working/submission/main.py\"\n    \n# TEAM 2 - ANSWERER\nagent4 = \"/kaggle/working/simulation/agent4.py\"\nif GEMMA_AS_ANSWERER:\n    agent4 = \"/kaggle/working/submission/main.py\"\n    \nenv.reset()\nlog = env.run([agent1, agent2, agent3, agent4])\n\nenv.render(mode=\"ipython\", width=600, height=500)\n\nimport gc, torch\ndel make, env, log\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:25:58.216169Z","iopub.execute_input":"2024-07-21T04:25:58.216656Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"\n#########################\n### Round 1\n#########################\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 0, 'questions': [], 'guesses': [], 'answers': [], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\nOUTPUT = 'Is it Duct tape?'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074464a3b24442aca1d0302e151c7809"}},"metadata":{}},{"name":"stderr","text":"2024-07-21 04:26:35.933292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 04:26:35.933390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 04:26:36.075130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"Initializing model\nresponse='Is it a thing?'\n\nTeam 1 - Answerer - Agent Random\nINPUT = {'remainingOverageTime': 300, 'questions': ['Is it Duct tape?'], 'guesses': [], 'answers': [], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'miami florida', 'category': 'place', 'step': 1}\nOUTPUT = 'no'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c78f7bea9fb4660a54bc60118d73e71"}},"metadata":{}},{"name":"stdout","text":"Initializing model\nresponse='no'\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 2, 'questions': ['Is it Duct tape?'], 'guesses': [], 'answers': ['no'], 'role': 'guesser', 'turnType': 'guess', 'keyword': '', 'category': ''}\nOUTPUT = 'Ointment'\nresponse='no'\n\n#########################\n### Round 2\n#########################\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 3, 'questions': ['Is it Duct tape?'], 'guesses': ['Ointment'], 'answers': ['no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\nOUTPUT = 'Is it wellington new zealand?'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}