{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Prerequisites\n","Install HuggingFace packages and create submission directory."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-20T06:19:56.936515Z","iopub.status.busy":"2024-07-20T06:19:56.936140Z","iopub.status.idle":"2024-07-20T06:20:32.742164Z","shell.execute_reply":"2024-07-20T06:20:32.741284Z","shell.execute_reply.started":"2024-07-20T06:19:56.936482Z"},"trusted":true},"outputs":[],"source":["%%bash\n","cd /kaggle/working\n","pip install -qU transformers bitsandbytes accelerate\n","mkdir -p /kaggle/working/submission/lib/gemma/\n","# mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"]},{"cell_type":"markdown","metadata":{},"source":["### HuggingFace Login\n","\n","Add HugginFace access token to secrets. You can find it in `Add-ons -> secrets`"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:20:32.745183Z","iopub.status.busy":"2024-07-20T06:20:32.744167Z","iopub.status.idle":"2024-07-20T06:20:33.553774Z","shell.execute_reply":"2024-07-20T06:20:33.552755Z","shell.execute_reply.started":"2024-07-20T06:20:32.745142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import huggingface_hub\n","from kaggle_secrets import UserSecretsClient\n","\n","huggingface_hub.login(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))"]},{"cell_type":"markdown","metadata":{},"source":["### Download Model via HuggingFace\n","In this notebook, we are using gemma-2-9b model with 4-bit quantization."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:20:33.555298Z","iopub.status.busy":"2024-07-20T06:20:33.554993Z","iopub.status.idle":"2024-07-20T06:23:42.502737Z","shell.execute_reply":"2024-07-20T06:23:42.501854Z","shell.execute_reply.started":"2024-07-20T06:20:33.555270Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04adccd0068b40b4af19aacf51176af4","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20f991574f0c4e9085d160c41ecb0922","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9421aaa55cf74be19f1df8e3e05eb6b0","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e2393ad921f48389184cc79ee2dfe77","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de9a87f2f28d4a7b8da7e69e86a81d84","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af31b20ef5f84d9ab120453dc671e7aa","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c36d4bc636944d48b96afa0bb1462d54","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90cb9dbf465840f6b3e8d07f2f6eeab1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5ff56a92704491cbae97cba554990b3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2487e2e417647fe8dab8a6091f70c4f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97504f2e09d644089e8e7b96bbf661cb","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f664ec4805fb4bb4a880985925911e92","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e318c5a8308499bac4f64d327132cad","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# pip install accelerate\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)\n","\n","model_id = \"google/gemma-2-9b-it\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_quanty_type = \"fp4\", \n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quanty = True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config = bnb_config,\n","    torch_dtype = torch.float16,\n","    device_map = \"auto\",\n","    trust_remote_code = True,\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Save Model\n","Save the loaded model and tokenizer in the submission directory.\n","Remove the model and tokenizer from the memory."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:23:42.505145Z","iopub.status.busy":"2024-07-20T06:23:42.504686Z","iopub.status.idle":"2024-07-20T06:24:00.157371Z","shell.execute_reply":"2024-07-20T06:24:00.156385Z","shell.execute_reply.started":"2024-07-20T06:23:42.505115Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('/kaggle/working/submission/lib/gemma/tokenizer_config.json',\n"," '/kaggle/working/submission/lib/gemma/special_tokens_map.json',\n"," '/kaggle/working/submission/lib/gemma/tokenizer.model',\n"," '/kaggle/working/submission/lib/gemma/added_tokens.json',\n"," '/kaggle/working/submission/lib/gemma/tokenizer.json')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"/kaggle/working/submission/lib/gemma\")\n","tokenizer.save_pretrained(\"/kaggle/working/submission/lib/gemma\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T05:09:08.865132Z","iopub.status.busy":"2024-07-20T05:09:08.864617Z","iopub.status.idle":"2024-07-20T05:09:09.307470Z","shell.execute_reply":"2024-07-20T05:09:09.306678Z","shell.execute_reply.started":"2024-07-20T05:09:08.865098Z"},"trusted":true},"outputs":[],"source":["import gc, torch\n","del model, tokenizer\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["## Agent"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T05:09:09.309674Z","iopub.status.busy":"2024-07-20T05:09:09.309266Z","iopub.status.idle":"2024-07-20T05:09:10.156821Z","shell.execute_reply":"2024-07-20T05:09:10.155884Z","shell.execute_reply.started":"2024-07-20T05:09:09.309640Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing submission/main.py\n"]}],"source":["%%writefile submission/main.py\n","# Setup\n","import os\n","import sys\n","\n","# **IMPORTANT:** Set up your system path like this to make your code work\n","# both in notebooks and in the simulations environment.\n","KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n","if os.path.exists(KAGGLE_AGENT_PATH):\n","    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n","else:\n","    sys.path.insert(0, \"/kaggle/working/submission/lib/gemma\")\n","\n","import contextlib\n","import os\n","import sys\n","from pathlib import Path\n","\n","# import torch\n","# from gemma.config import GemmaConfig, get_model_config\n","# from gemma.model import GemmaForCausalLM\n","# from gemma.tokenizer import Tokenizer\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)\n","\n","if os.path.exists(KAGGLE_AGENT_PATH):\n","    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma\")\n","else:\n","    MODEL_PATH = \"/kaggle/working/submission/lib/gemma\"\n","\n","# Prompt Formatting\n","import itertools\n","from typing import Iterable\n","\n","\n","class GemmaFormatter:\n","    _start_token = '<start_of_turn>'\n","    _end_token = '<end_of_turn>'\n","\n","    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n","        self._system_prompt = system_prompt\n","        self._few_shot_examples = few_shot_examples\n","        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n","        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n","        self.reset()\n","\n","    def __repr__(self):\n","        return self._state\n","\n","    def user(self, prompt):\n","        self._state += self._turn_user.format(prompt)\n","        return self\n","\n","    def model(self, prompt):\n","        self._state += self._turn_model.format(prompt)\n","        return self\n","\n","    def start_user_turn(self):\n","        self._state += f\"{self._start_token}user\\n\"\n","        return self\n","\n","    def start_model_turn(self):\n","        self._state += f\"{self._start_token}model\\n\"\n","        return self\n","\n","    def end_turn(self):\n","        self._state += f\"{self._end_token}\\n\"\n","        return self\n","\n","    def reset(self):\n","        self._state = \"\"\n","        if self._system_prompt is not None:\n","            self.user(self._system_prompt)\n","        if self._few_shot_examples is not None:\n","            self.apply_turns(self._few_shot_examples, start_agent='user')\n","        return self\n","\n","    def apply_turns(self, turns: Iterable, start_agent: str):\n","        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n","        formatters = itertools.cycle(formatters)\n","        for fmt, turn in zip(formatters, turns):\n","            fmt(turn)\n","        return self\n","\n","\n","# Agent Definitions\n","import re\n","\n","\n","@contextlib.contextmanager\n","def _set_default_tensor_type(dtype: torch.dtype):\n","    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n","    torch.set_default_dtype(dtype)\n","    yield\n","    torch.set_default_dtype(torch.float)\n","\n","\n","class GemmaAgent:\n","    def __init__(self, model_path=MODEL_PATH, device='cuda:0', system_prompt=None, few_shot_examples=None):\n","        self._device = torch.device(device)\n","        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n","\n","        print(\"Initializing model\")\n","        # bnb_config = BitsAndBytesConfig(\n","        #     load_in_4bit = True,\n","        #     bnb_4bit_quanty_type = \"fp4\", \n","        #     bnb_4bit_compute_dtype=torch.float16,\n","        #     bnb_4bit_use_double_quanty = True,\n","        # )\n","\n","        self.model = AutoModelForCausalLM.from_pretrained(\n","            model_path,\n","            # quantization_config = bnb_config,\n","            # torch_dtype = torch.float16,\n","            device_map = \"auto\",\n","            # trust_remote_code = True,\n","        )\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        \n","#         model_config = get_model_config(variant)\n","#         model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n","#         model_config.quant = \"quant\" in variant\n","\n","#         with _set_default_tensor_type(model_config.get_dtype()):\n","#             model = GemmaForCausalLM(model_config)\n","#             ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n","#             model.load_weights(ckpt_path)\n","#             self.model = model.to(self._device).eval()\n","\n","    def __call__(self, obs, *args):\n","        self._start_session(obs)\n","        prompt = str(self.formatter)\n","        response = self._call_llm(prompt)\n","        response = self._parse_response(response, obs)\n","        print(f\"{response=}\")\n","        return response\n","\n","    def _start_session(self, obs: dict):\n","        raise NotImplementedError\n","\n","    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n","        if sampler_kwargs is None:\n","            sampler_kwargs = {\n","                'temperature': 0.01,\n","                'top_p': 0.1,\n","                'top_k': 1,\n","        }\n","        # response = self.model.generate(\n","        #     prompt,\n","        #     device=self._device,\n","        #     output_len=max_new_tokens,\n","        #     **sampler_kwargs,\n","        # )\n","        # return response\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","        outputs = self.model.generate(input_ids=inputs.to(self.model.device), max_new_tokens=max_new_tokens, kwargs=sampler_kwargs)\n","        return self.tokenizer.decode(outputs[0])\n","\n","    def _parse_keyword(self, response: str):\n","        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n","        if match is None:\n","            keyword = ''\n","        else:\n","            keyword = match.group().lower()\n","        return keyword\n","\n","    def _parse_response(self, response: str, obs: dict):\n","        raise NotImplementedError\n","\n","\n","def interleave_unequal(x, y):\n","    return [\n","        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n","    ]\n","\n","\n","class GemmaQuestionerAgent(GemmaAgent):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","    def _start_session(self, obs):\n","        self.formatter.reset()\n","        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n","        turns = interleave_unequal(obs.questions, obs.answers)\n","        self.formatter.apply_turns(turns, start_agent='model')\n","        if obs.turnType == 'ask':\n","            self.formatter.user(\"Please ask a yes-or-no question.\")\n","        elif obs.turnType == 'guess':\n","            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n","        self.formatter.start_model_turn()\n","\n","    def _parse_response(self, response: str, obs: dict):\n","        if obs.turnType == 'ask':\n","            match = re.search(\".+?\\?\", response.replace('*', ''))\n","            if match is None:\n","                question = \"Is it a place?\"\n","            else:\n","                question = match.group()\n","            return question\n","        elif obs.turnType == 'guess':\n","            guess = self._parse_keyword(response)\n","            return guess\n","        else:\n","            raise ValueError(\"Unknown turn type:\", obs.turnType)\n","\n","\n","class GemmaAnswererAgent(GemmaAgent):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","    def _start_session(self, obs):\n","        self.formatter.reset()\n","        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n","        turns = interleave_unequal(obs.questions, obs.answers)\n","        self.formatter.apply_turns(turns, start_agent='user')\n","        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n","        self.formatter.start_model_turn()\n","\n","    def _parse_response(self, response: str, obs: dict):\n","        answer = self._parse_keyword(response)\n","        return 'yes' if 'yes' in answer else 'no'\n","\n","\n","# Agent Creation\n","system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n","\n","few_shot_examples = [\n","    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n","    \"Is it a thing?\", \"**no**\",\n","    \"Is is a place?\", \"**yes**\",\n","    \"Is it a country?\", \"**yes**\",\n","    \"Does it start with f?\", \"**yes** Now guess the keyword.\",\n","    \"**France**\", \"Correct!\",\n","]\n","\n","\n","# **IMPORTANT:** Define agent as a global so you only have to load\n","# the agent you need. Loading both will likely lead to OOM.\n","agent = None\n","\n","\n","def get_agent(name: str):\n","    global agent\n","    \n","    if agent is None and name == 'questioner':\n","        agent = GemmaQuestionerAgent(\n","            device='cuda:0',\n","            system_prompt=system_prompt,\n","            few_shot_examples=few_shot_examples,\n","        )\n","    elif agent is None and name == 'answerer':\n","        agent = GemmaAnswererAgent(\n","            device='cuda:0',\n","            system_prompt=system_prompt,\n","            few_shot_examples=few_shot_examples,\n","        )\n","    assert agent is not None, \"Agent not initialized.\"\n","\n","    return agent\n","\n","\n","def agent_fn(obs, cfg):\n","    if obs.turnType == \"ask\":\n","        response = get_agent('questioner')(obs)\n","    elif obs.turnType == \"guess\":\n","        response = get_agent('questioner')(obs)\n","    elif obs.turnType == \"answer\":\n","        response = get_agent('answerer')(obs)\n","    if response is None or len(response) <= 1:\n","        return \"yes\"\n","    else:\n","        return response"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!apt install pigz pv > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!tar --use-compress-program='pigz --fast --recursive | pv' -cf /kaggle/working/submission.tar.gz -C /kaggle/working/submission "]},{"cell_type":"markdown","metadata":{},"source":["## Simulate Game"]},{"cell_type":"markdown","metadata":{},"source":["### Load test data\n","Download the latest keywords.py from [kaggle-environments](https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/llm_20_questions/keywords.py) github repo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%bash\n","\n","wget -O keywords.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n","mkdir -p /kaggle/working/simulation/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import pandas as pd\n","import numpy as np\n","from keywords import KEYWORDS_JSON\n","\n","def create_keyword_df(KEYWORDS_JSON):\n","    json_data = json.loads(KEYWORDS_JSON)\n","\n","    keyword_list = []\n","    category_list = []\n","    alts_list = []\n","\n","    for i in range(len(json_data)):\n","        for j in range(len(json_data[i]['words'])):\n","            keyword = json_data[i]['words'][j]['keyword']\n","            keyword_list.append(keyword)\n","            category_list.append(json_data[i]['category'])\n","            alts_list.append(json_data[i]['words'][j]['alts'])\n","\n","    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n","    data_pd['keyword'] = keyword_list\n","    data_pd['category'] = category_list\n","    data_pd['alts'] = alts_list\n","    \n","    return data_pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keywords = create_keyword_df(KEYWORDS_JSON)\n","# keywords_df.head(5)\n","keywords.tail(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keywords[\"category\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keywords.to_csv(\"/kaggle/working/simulation/keywords.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Create Agents\n","2 vs 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile /kaggle/working/simulation/agent1.py\n","\n","import pandas as pd\n","import numpy as np\n","\n","keywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n","\n","def agent_fn(obs, cfg):\n","    global keywords\n","    \n","    # DISPLAY ROUND NUMBER\n","    k = len( obs.questions )\n","    if obs.turnType == \"ask\":\n","        print()\n","        print(\"#\"*25)\n","        print(f\"### Round {k+1}\")\n","        print(\"#\"*25)\n","\n","    # DISPLAY AGENT NAME AND JSON INPUT\n","    name = \"Team 1 - Questioner - Agent Random\"\n","    print(f\"\\n{name}\\nINPUT =\",obs)\n","    \n","    # GENERATE RESPONSE\n","    keyword = np.random.choice(keywords)\n","    if obs.turnType == \"ask\":\n","        response = f\"Is it {keyword}?\"\n","    else: #obs.turnType == \"guess\"\n","        response = keyword\n","        if obs.answers[-1] == \"yes\":\n","            response = obs.questions[-1].rsplit(\" \",1)[1][:-1]\n","    print(f\"OUTPUT = '{response}'\")\n","\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile /kaggle/working/simulation/agent2.py\n","\n","import numpy as np\n","\n","def agent_fn(obs, cfg):\n","    \n","    # DISPLAY AGENT NAME AND JSON INPUT\n","    name = \"Team 1 - Answerer - Agent Random\"\n","    print(f\"\\n{name}\\nINPUT =\",obs)\n","    \n","    # GENERATE RESPONSE\n","    response = \"no\"\n","    #response = np.random.choice([\"yes\",\"no\"])\n","    if obs.keyword.lower() in obs.questions[-1].lower():\n","        response = \"yes\"\n","    print(f\"OUTPUT = '{response}'\")\n","\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile /kaggle/working/simulation/agent3.py\n","\n","import pandas as pd\n","import numpy as np\n","\n","keywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n","\n","def agent_fn(obs, cfg):\n","    global keywords\n","    \n","    # DISPLAY AGENT NAME AND JSON INPUT\n","    name = \"Team 2 - Questioner - Agent Random\"\n","    print(f\"\\n{name}\\nINPUT =\",obs)\n","    \n","    # GENERATE RESPONSE\n","    keyword = np.random.choice(keywords)\n","    if obs.turnType == \"ask\":\n","        response = f\"Is it {keyword}?\"\n","    else: #obs.turnType == \"guess\"\n","        response = keyword\n","        if obs.answers[-1] == \"yes\":\n","            response = obs.questions[-1].rsplit(\" \",1)[1][:-1]\n","    print(f\"OUTPUT = '{response}'\")\n","\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile /kaggle/working/simulation/agent4.py\n","\n","import numpy as np\n","\n","def agent_fn(obs, cfg):\n","    \n","    # DISPLAY AGENT NAME AND JSON INPUT\n","    name = \"Team 2 - Answerer - Agent Random\"\n","    print(f\"\\n{name}\\nINPUT =\",obs)\n","    \n","    # GENERATE RESPONSE\n","    response = \"no\"\n","    #response = np.random.choice([\"yes\",\"no\"])\n","    if obs.keyword.lower() in obs.questions[-1].lower():\n","        response = \"yes\"\n","    print(f\"OUTPUT = '{response}'\")\n","\n","    return response"]},{"cell_type":"markdown","metadata":{},"source":["### Create Environment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q pygame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GEMMA_AS_QUESTIONER = True\n","GEMMA_AS_ANSWERER = True\n","\n","from kaggle_environments import make\n","env = make(\"llm_20_questions\", debug=True)\n","\n","# TEAM 1\n","agent1 = \"/kaggle/working/simulation/agent1.py\"\n","agent2 = \"/kaggle/working/simulation/agent2.py\"\n","\n","# TEAM 2 - QUESTIONER\n","agent3 = \"/kaggle/working/simulation/agent3.py\"\n","if GEMMA_AS_QUESTIONER:\n","    agent3 = \"/kaggle/working/submission/main.py\"\n","    \n","# TEAM 2 - ANSWERER\n","agent4 = \"/kaggle/working/simulation/agent4.py\"\n","if GEMMA_AS_ANSWERER:\n","    agent4 = \"/kaggle/working/submission/main.py\"\n","    \n","env.reset()\n","log = env.run([agent1, agent2, agent3, agent4])\n","\n","import gc, torch\n","del make, env, log\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8550470,"sourceId":61247,"sourceType":"competition"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
