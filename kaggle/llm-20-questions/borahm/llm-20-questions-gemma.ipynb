{
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 61247,
     "databundleVersionId": 8550470,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Prerequisites\nInstall HuggingFace packages and create submission directory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf /kaggle/working/submission # WARNING: 삭제가 필요한 경우에만 사용\n",
    "\n",
    "!mkdir /kaggle/working/submission"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:11:31.796703Z",
     "iopub.execute_input": "2024-07-21T04:11:31.797132Z",
     "iopub.status.idle": "2024-07-21T04:11:32.873434Z",
     "shell.execute_reply.started": "2024-07-21T04:11:31.797086Z",
     "shell.execute_reply": "2024-07-21T04:11:32.872315Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import os, sys, importlib, subprocess\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/working/submission/lib\") # 설치 경로 추가\n",
    "\n",
    "def uninstall_package(package_name):\n",
    "    \"\"\"패키지를 삭제하는 함수\"\"\"\n",
    "    os.system(f\"pip uninstall -y {package_name}\")\n",
    "\n",
    "os.system(\"pip install -q -U pip\")\n",
    "\n",
    "packages = [\"bitsandbytes\", \"accelerate\", \"transformers\"] # 설치할 패키지 목록\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        print(f\"{pkg}을(를) 설치합니다.\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"## {pkg} 설치에 실패하였으므로 프로세스를 종료합니다.\")\n",
    "        sys.exit(1)  # 비정상 종료\n",
    "     \n",
    "os.system(\"pip freeze | egrep 'bitsandbytes|accelerate|transformers'\") # 설치된 특정 패키지 목록을 출력합니다.\n",
    "\n",
    "os.system(\"pip cache purge\") # pip 캐시를 정리합니다."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:11:33.833410Z",
     "iopub.execute_input": "2024-07-21T04:11:33.834092Z",
     "iopub.status.idle": "2024-07-21T04:14:26.929371Z",
     "shell.execute_reply.started": "2024-07-21T04:11:33.834053Z",
     "shell.execute_reply": "2024-07-21T04:14:26.928136Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nucx-py 0.38.0 requires libucx<1.16,>=1.15.0, which is not installed.\nucxx 0.38.0 requires libucx>=1.15.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndatasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2024.6.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nrmm 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2024.6.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\ntensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Files removed: 242\nCPU times: user 11.7 ms, sys: 4.73 ms, total: 16.5 ms\nWall time: 2min 53s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HuggingFace Login\n",
    "\n",
    "Add HuggingFace access token to secrets. You can find it in `Add-ons -> secrets`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import huggingface_hub\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "huggingface_hub.login(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:14:26.931413Z",
     "iopub.execute_input": "2024-07-21T04:14:26.932155Z",
     "iopub.status.idle": "2024-07-21T04:14:27.609923Z",
     "shell.execute_reply.started": "2024-07-21T04:14:26.932117Z",
     "shell.execute_reply": "2024-07-21T04:14:27.609033Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Download Model via HuggingFace\nIn this notebook, we are using gemma-2-9b model with 4-bit quantization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False) # 메모리 효율적인 SDP(Sparse Dense Packing) 비활성화\n",
    "torch.backends.cuda.enable_flash_sdp(False) # Flash SDP 비활성화\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\" # 모델 식별자 설정\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,  # 4비트 로딩 활성화\n",
    "    bnb_4bit_quanty_type = \"fp4\",  # 4비트 양자화 타입을 fp4로 설정\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산에 사용할 데이터 타입을 float16으로 설정\n",
    "    bnb_4bit_use_double_quanty = True,  # 이중 양자화 사용\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained( # 사전 훈련된 모델 로드\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,  # 양자화 설정 적용\n",
    "    torch_dtype = torch.float16,  # 모델의 데이터 타입을 float16으로 설정\n",
    "    device_map = \"auto\",  # 장치 맵 자동 설정\n",
    "    trust_remote_code = True,  # 원격 코드 신뢰 설정\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # 토크나이저 로드\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:14:27.611333Z",
     "iopub.execute_input": "2024-07-21T04:14:27.611752Z",
     "iopub.status.idle": "2024-07-21T04:17:42.100933Z",
     "shell.execute_reply.started": "2024-07-21T04:14:27.611706Z",
     "shell.execute_reply": "2024-07-21T04:17:42.100031Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "Unused kwargs: ['bnb_4bit_quanty_type', 'bnb_4bit_use_double_quanty']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26507a6f7a9148d2b57ed668f80876b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c6eb143a1343a9a8d0acd92967e29a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12f1110d678d4b528f9cdf8d9b2eea08"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cbb31fd2d41400c9cb9b12303a0145f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14c097e39fbe4969a7223438cf7c7d9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bc2ad6071224b189dd8501e5638bf8a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af4718a644154d679064de0a21fa5469"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5eb9cc41d2834747baedcfe3b1d5981b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cdf6b9d35aa4954837631880fc9e079"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d508428df4c54923b1aca7f652bccd2d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c324f70a9d2418aa5d66e0b15e67233"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "965b899e82dc418e87c1528fb5f1b540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "087a2e98d4554f41bb1ae977064da25a"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Save Model\nSave the loaded model and tokenizer in the submission directory.\nRemove the model and tokenizer from the memory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"/kaggle/working/submission/model\") # 모델을 지정된 경로에 저장합니다.\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\") # 토크나이저를 같은 경로에 저장합니다."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:17:42.103067Z",
     "iopub.execute_input": "2024-07-21T04:17:42.103565Z",
     "iopub.status.idle": "2024-07-21T04:17:58.646486Z",
     "shell.execute_reply.started": "2024-07-21T04:17:42.103537Z",
     "shell.execute_reply": "2024-07-21T04:17:58.645502Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "('/kaggle/working/submission/model/tokenizer_config.json',\n '/kaggle/working/submission/model/special_tokens_map.json',\n '/kaggle/working/submission/model/tokenizer.model',\n '/kaggle/working/submission/model/added_tokens.json',\n '/kaggle/working/submission/model/tokenizer.json')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gc, torch\n",
    "del model, tokenizer # 사용이 끝난 model과 tokenizer 객체를 메모리에서 삭제합니다.\n",
    "gc.collect() # 가비지 컬렉터를 수동으로 호출하여, 참조되지 않는 객체들을 메모리에서 제거합니다.\n",
    "torch.cuda.empty_cache() # PyTorch의 CUDA 캐시를 비워 GPU 메모리를 확보합니다."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:17:58.647922Z",
     "iopub.execute_input": "2024-07-21T04:17:58.648565Z",
     "iopub.status.idle": "2024-07-21T04:17:59.115008Z",
     "shell.execute_reply.started": "2024-07-21T04:17:58.648527Z",
     "shell.execute_reply": "2024-07-21T04:17:59.114023Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Agent",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **중요:** 시스템 경로를 아래와 같이 설정하여 코드가 노트북과 시뮬레이션 환경 양쪽에서 모두 작동하도록 합니다.\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\" # Kaggle 에이전트가 위치한 경로 정의\n",
    "\n",
    "# Kaggle 에이전트 경로가 존재하는지 확인하고, 'lib' 디렉토리를 시스템 경로에 추가합니다.\n",
    "# 이는 코드가 Kaggle 노트북과 Kaggle 시뮬레이션 환경 양쪽에서 모두 작동할 수 있도록 보장합니다.\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False) # 메모리 효율적인 SDP를 비활성화합니다.\n",
    "torch.backends.cuda.enable_flash_sdp(False) # Flash SDP를 비활성화합니다.\n",
    "\n",
    "# KAGGLE_AGENT_PATH가 존재하는지 확인하고, 존재하면 모델 경로를 설정합니다.\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "# 프롬프트 포매팅을 위한 코드\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        # 생성자에서 시스템 프롬프트와 예시를 초기화합니다.\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 객체를 문자열로 표현할 때 상태를 반환합니다.\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        # 사용자 프롬프트를 상태에 추가합니다.\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        # 모델 프롬프트를 상태에 추가합니다.\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        # 사용자 턴 시작을 상태에 추가합니다.\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        # 모델 턴 시작을 상태에 추가합니다.\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        # 턴 종료를 상태에 추가합니다.\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        # 상태를 초기화하고, 시스템 프롬프트와 예시가 있다면 적용합니다.\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 주어진 턴들을 순서대로 적용합니다. 시작 에이전트에 따라 순서가 결정됩니다.\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent 정의\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager # 텐서 타입을 임시로 설정하는 컨텍스트 매니저\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"주어진 dtype으로 기본 torch dtype을 설정합니다.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, model_path=MODEL_PATH, device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._device = torch.device(device) # 디바이스 설정\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples) # 포맷터 초기화\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map = \"auto\",\n",
    "        ) # 모델 로드\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path) # 토크나이저 로드\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs) # 세션 시작\n",
    "        prompt = str(self.formatter) # 프롬프트 생성\n",
    "        response = self._call_llm(prompt) # LLM 호출\n",
    "        response = self._parse_response(response, obs) # 응답 파싱\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        # 세션 시작 메서드 (구현 필요)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        # LLM 호출 메서드\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, kwargs=sampler_kwargs)\n",
    "        return self.tokenizer.decode(outputs[0])\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 응답에서 키워드 파싱\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 응답 파싱 메서드 (구현 필요)\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"두 리스트의 요소를 교차로 결합합니다. 리스트 길이가 다를 경우 None을 제외하고 결합합니다.\"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # GemmaAgent 클래스를 상속받아 초기화합니다. 추가 인자들은 부모 클래스의 초기화 메서드로 전달됩니다.\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # 대화 세션을 시작하기 전에 formatter를 초기화합니다.\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\") # 사용자에게 20 Questions 게임을 시작한다고 알리고, 사용자가 질문자 역할임을 알립니다.\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 질문과 답변을 교차로 배치합니다.\n",
    "        self.formatter.apply_turns(turns, start_agent='model') # 교차 배치된 질문과 답변을 formatter에 적용합니다. 모델이 첫 번째로 시작합니다.\n",
    "        \n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\") # 사용자에게 예/아니오 질문을 요청합니다.\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\") # 사용자에게 키워드를 추측하도록 요청합니다. 추측은 이중 별표로 둘러싸야 합니다.\n",
    "        self.formatter.start_model_turn() # 모델의 차례를 시작합니다.\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', '')) # 사용자의 응답에서 질문을 찾습니다. 별표는 제거됩니다.\n",
    "            if match is None:\n",
    "                question = \"Is it a place?\" # 질문을 찾지 못한 경우 기본 질문을 사용합니다.\n",
    "            else:\n",
    "                question = match.group() # 찾은 질문을 반환합니다.\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response) # 사용자의 응답에서 키워드를 추출합니다.\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType) # 알 수 없는 차례 유형에 대한 예외를 발생시킵니다.\n",
    "\n",
    "            \n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # GemmaAgent 클래스를 상속받아 초기화합니다. 추가 인자들은 부모 클래스의 초기화 메서드로 전달됩니다.\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # 대화 세션을 시작하기 전에 formatter를 초기화합니다.\n",
    "        \n",
    "        # 사용자에게 20 Questions 게임을 시작한다고 알리고, 사용자가 답변자 역할임을 알립니다. 키워드와 카테고리도 알립니다.\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        \n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 질문과 답변을 교차로 배치합니다.\n",
    "        self.formatter.apply_turns(turns, start_agent='user') # 교차 배치된 질문과 답변을 formatter에 적용합니다. 사용자가 첫 번째로 시작합니다.\n",
    "        \n",
    "        # 사용자에게 키워드와 카테고리에 관한 질문에 예/아니오로 답변하도록 요청합니다. 답변은 이중 별표로 둘러싸야 합니다.\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn() # 모델의 차례를 시작합니다.\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response) # 사용자의 응답에서 키워드를 추출합니다.\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a thing?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes**\",\n",
    "    \"Does it start with f?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **중요:** 에이전트를 전역 변수로 정의하여 필요한 에이전트만 로드합니다.\n",
    "# 두 에이전트를 모두 로드하면 OOM(Out of Memory)이 발생할 가능성이 높습니다.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # 에이전트가 None이고, 요청된 이름이 'questioner'인 경우\n",
    "    if agent is None and name == 'questioner':\n",
    "        # 질문자 에이전트를 초기화합니다.\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    # 에이전트가 None이고, 요청된 이름이 'answerer'인 경우\n",
    "    elif agent is None and name == 'answerer':\n",
    "        # 답변자 에이전트를 초기화합니다.\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    # 에이전트가 초기화되지 않았다면 에러를 발생시킵니다.\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # 관찰된 턴 타입에 따라 적절한 에이전트를 호출합니다.\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "        \n",
    "    # 응답이 None이거나 길이가 1 이하인 경우 'yes'를 반환합니다.\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:18:49.984707Z",
     "iopub.execute_input": "2024-07-21T04:18:49.985110Z",
     "iopub.status.idle": "2024-07-21T04:18:49.997141Z",
     "shell.execute_reply.started": "2024-07-21T04:18:49.985071Z",
     "shell.execute_reply": "2024-07-21T04:18:49.996228Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "Writing /kaggle/working/submission/main.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!apt install pigz pv > /dev/null",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:19:38.764944Z",
     "iopub.execute_input": "2024-07-21T04:19:38.765363Z",
     "iopub.status.idle": "2024-07-21T04:19:45.596700Z",
     "shell.execute_reply.started": "2024-07-21T04:19:38.765334Z",
     "shell.execute_reply": "2024-07-21T04:19:45.595225Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!tar --use-compress-program='pigz --fast --recursive | pv' -cf /kaggle/working/submission.tar.gz -C /kaggle/working/submission .",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:20:12.765633Z",
     "iopub.execute_input": "2024-07-21T04:20:12.767060Z",
     "iopub.status.idle": "2024-07-21T04:23:30.681848Z",
     "shell.execute_reply.started": "2024-07-21T04:20:12.766994Z",
     "shell.execute_reply": "2024-07-21T04:23:30.680731Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "8.11GiB 0:03:16 [42.2MiB/s] [  <=>                                             ]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Simulate Game",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Load test data\nDownload the latest keywords.py from [kaggle-environments](https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/llm_20_questions/keywords.py) github repo",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "\n",
    "wget -O keywords.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "mkdir -p /kaggle/working/simulation/"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:05.556195Z",
     "iopub.execute_input": "2024-07-21T04:25:05.556633Z",
     "iopub.status.idle": "2024-07-21T04:25:05.809476Z",
     "shell.execute_reply.started": "2024-07-21T04:25:05.556599Z",
     "shell.execute_reply": "2024-07-21T04:25:05.808516Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": "--2024-07-21 04:25:05--  https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93552 (91K) [text/plain]\nSaving to: 'keywords.py'\n\n     0K .......... .......... .......... .......... .......... 54% 3.00M 0s\n    50K .......... .......... .......... .......... .         100% 27.6M=0.02s\n\n2024-07-21 04:25:05 (5.03 MB/s) - 'keywords.py' saved [93552/93552]\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keywords import KEYWORDS_JSON\n",
    "\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON) # JSON 문자열을 파이썬 객체로 변환\n",
    "\n",
    "    # 키워드, 카테고리, 대체어를 저장할 리스트 초기화\n",
    "    keyword_list = []\n",
    "    category_list = []\n",
    "    alts_list = []\n",
    "\n",
    "    # JSON 데이터를 순회하며 키워드, 카테고리, 대체어 정보 추출\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):\n",
    "            keyword = json_data[i]['words'][j]['keyword']\n",
    "            keyword_list.append(keyword)\n",
    "            category_list.append(json_data[i]['category'])\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])\n",
    "\n",
    "    # 추출한 정보를 이용해 pandas DataFrame 생성\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n",
    "    data_pd['keyword'] = keyword_list\n",
    "    data_pd['category'] = category_list\n",
    "    data_pd['alts'] = alts_list\n",
    "    \n",
    "    return data_pd\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:17.925113Z",
     "iopub.execute_input": "2024-07-21T04:25:17.925811Z",
     "iopub.status.idle": "2024-07-21T04:25:18.525138Z",
     "shell.execute_reply.started": "2024-07-21T04:25:17.925778Z",
     "shell.execute_reply": "2024-07-21T04:25:18.524304Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keywords = create_keyword_df(KEYWORDS_JSON) # KEYWORDS_JSON에서 키워드 데이터를 DataFrame으로 생성합니다.\n",
    "\n",
    "# keywords_df.head(5)\n",
    "keywords.tail(5)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:18.527087Z",
     "iopub.execute_input": "2024-07-21T04:25:18.528215Z",
     "iopub.status.idle": "2024-07-21T04:25:18.566600Z",
     "shell.execute_reply.started": "2024-07-21T04:25:18.528178Z",
     "shell.execute_reply": "2024-07-21T04:25:18.565560Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            keyword category                   alts\n1137          rhine    place                     []\n1138  yangtze river    place  [changjiang, yangtze]\n1139   yellow river    place             [huang he]\n1140  zambezi river    place              [zambezi]\n1141  yenisei river    place              [yenisei]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>category</th>\n      <th>alts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1137</th>\n      <td>rhine</td>\n      <td>place</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>yangtze river</td>\n      <td>place</td>\n      <td>[changjiang, yangtze]</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>yellow river</td>\n      <td>place</td>\n      <td>[huang he]</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>zambezi river</td>\n      <td>place</td>\n      <td>[zambezi]</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>yenisei river</td>\n      <td>place</td>\n      <td>[yenisei]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "keywords[\"category\"].unique() # keywords DataFrame의 \"category\" 열에서 고유한 값을 추출합니다.",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:18.567685Z",
     "iopub.execute_input": "2024-07-21T04:25:18.567941Z",
     "iopub.status.idle": "2024-07-21T04:25:18.577195Z",
     "shell.execute_reply.started": "2024-07-21T04:25:18.567918Z",
     "shell.execute_reply": "2024-07-21T04:25:18.576232Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['things', 'place'], dtype=object)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 키워드를 CSV 파일로 저장합니다. 인덱스는 포함하지 않습니다.\n",
    "keywords.to_csv('/kaggle/working/simulation/keywords.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:19.824742Z",
     "iopub.execute_input": "2024-07-21T04:25:19.825161Z",
     "iopub.status.idle": "2024-07-21T04:25:19.837550Z",
     "shell.execute_reply.started": "2024-07-21T04:25:19.825133Z",
     "shell.execute_reply": "2024-07-21T04:25:19.836657Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Create Agents\n2 vs 2",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/simulation/agent1.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 키워드 파일에서 키워드를 불러옵니다.\n",
    "keywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global keywords\n",
    "    \n",
    "    # 현재 라운드 번호를 표시합니다.\n",
    "    k = len( obs.questions )\n",
    "    if obs.turnType == \"ask\":\n",
    "        print()\n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Round {k+1}\")\n",
    "        print(\"#\"*25)\n",
    "\n",
    "    # 에이전트 이름과 JSON 입력을 표시합니다.\n",
    "    name = \"Team 1 - Questioner - Agent Random\"\n",
    "    print(f\"\\n{name}\\nINPUT =\",obs)\n",
    "    \n",
    "    # 응답을 생성합니다.\n",
    "    keyword = np.random.choice(keywords)\n",
    "    if obs.turnType == \"ask\": # 질문 차례인 경우\n",
    "        response = f\"Is it {keyword}?\" # \"Is it {선택된 키워드}?\" 형태의 질문 생성\n",
    "    else: #obs.turnType == \"guess\"\n",
    "        response = keyword # 선택된 키워드를 응답으로 설정\n",
    "        if obs.answers[-1] == \"yes\": # 마지막 답변이 'yes'인 경우\n",
    "            response = obs.questions[-1].rsplit(\" \",1)[1][:-1] # 마지막 질문에서 키워드를 추출하여 응답으로 설정\n",
    "    print(f\"OUTPUT = '{response}'\") # 생성된 응답 출력\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:31.473042Z",
     "iopub.execute_input": "2024-07-21T04:25:31.473458Z",
     "iopub.status.idle": "2024-07-21T04:25:31.480425Z",
     "shell.execute_reply.started": "2024-07-21T04:25:31.473413Z",
     "shell.execute_reply": "2024-07-21T04:25:31.479371Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "Writing /kaggle/working/simulation/agent1.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/simulation/agent2.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \n",
    "    # DISPLAY AGENT NAME AND JSON INPUT\n",
    "    name = \"Team 1 - Answerer - Agent Random\"\n",
    "    print(f\"\\n{name}\\nINPUT =\",obs)\n",
    "    \n",
    "    # GENERATE RESPONSE\n",
    "    response = \"no\"\n",
    "    #response = np.random.choice([\"yes\",\"no\"])\n",
    "    # 질문에 키워드가 포함되어 있으면 응답을 \"yes\"로 변경합니다.\n",
    "    if obs.keyword.lower() in obs.questions[-1].lower():\n",
    "        response = \"yes\"\n",
    "    print(f\"OUTPUT = '{response}'\")\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:31.617752Z",
     "iopub.execute_input": "2024-07-21T04:25:31.618080Z",
     "iopub.status.idle": "2024-07-21T04:25:31.623950Z",
     "shell.execute_reply.started": "2024-07-21T04:25:31.618036Z",
     "shell.execute_reply": "2024-07-21T04:25:31.622820Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "Writing /kaggle/working/simulation/agent2.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/simulation/agent3.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "keywords = pd.read_csv(\"/kaggle/working/simulation/keywords.csv\").keyword.values\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global keywords\n",
    "    \n",
    "    # DISPLAY AGENT NAME AND JSON INPUT\n",
    "    name = \"Team 2 - Questioner - Agent Random\"\n",
    "    print(f\"\\n{name}\\nINPUT =\",obs)\n",
    "    \n",
    "    # GENERATE RESPONSE\n",
    "    # 응답 생성\n",
    "    keyword = np.random.choice(keywords)  # 키워드 중 하나를 무작위로 선택\n",
    "    if obs.turnType == \"ask\":  # 질문 차례인 경우\n",
    "        response = f\"Is it {keyword}?\"  # \"Is it {선택된 키워드}?\" 형태의 질문 생성\n",
    "    else:  # 추측 차례인 경우\n",
    "        response = keyword  # 선택된 키워드를 응답으로 설정\n",
    "        if obs.answers[-1] == \"yes\":  # 마지막 답변이 'yes'인 경우\n",
    "            response = obs.questions[-1].rsplit(\" \",1)[1][:-1]  # 마지막 질문에서 키워드를 추출하여 응답으로 설정\n",
    "    print(f\"OUTPUT = '{response}'\")  # 생성된 응답 출력\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:31.792211Z",
     "iopub.execute_input": "2024-07-21T04:25:31.792523Z",
     "iopub.status.idle": "2024-07-21T04:25:31.799174Z",
     "shell.execute_reply.started": "2024-07-21T04:25:31.792494Z",
     "shell.execute_reply": "2024-07-21T04:25:31.798164Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "Writing /kaggle/working/simulation/agent3.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/simulation/agent4.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \n",
    "    # DISPLAY AGENT NAME AND JSON INPUT\n",
    "    name = \"Team 2 - Answerer - Agent Random\"\n",
    "    print(f\"\\n{name}\\nINPUT =\",obs)\n",
    "    \n",
    "    # GENERATE RESPONSE\n",
    "    response = \"no\"\n",
    "    #response = np.random.choice([\"yes\",\"no\"])\n",
    "    # 질문에 키워드가 포함되어 있으면 응답을 \"yes\"로 변경합니다.\n",
    "    if obs.keyword.lower() in obs.questions[-1].lower():\n",
    "        response = \"yes\"\n",
    "    print(f\"OUTPUT = '{response}'\")\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:32.345032Z",
     "iopub.execute_input": "2024-07-21T04:25:32.345411Z",
     "iopub.status.idle": "2024-07-21T04:25:32.351506Z",
     "shell.execute_reply.started": "2024-07-21T04:25:32.345385Z",
     "shell.execute_reply": "2024-07-21T04:25:32.350492Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Writing /kaggle/working/simulation/agent4.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Create Environment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q pygame",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:35.071770Z",
     "iopub.execute_input": "2024-07-21T04:25:35.072538Z",
     "iopub.status.idle": "2024-07-21T04:25:51.017306Z",
     "shell.execute_reply.started": "2024-07-21T04:25:35.072500Z",
     "shell.execute_reply": "2024-07-21T04:25:51.015868Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "GEMMA_AS_QUESTIONER = True # GEMMA가 질문자로 활동하는지 여부\n",
    "GEMMA_AS_ANSWERER = True # GEMMA가 답변자로 활동하는지 여부\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\", debug=True) # 'llm_20_questions' 환경을 생성하고 디버그 모드 활성화\n",
    "\n",
    "# TEAM 1\n",
    "agent1 = \"/kaggle/working/simulation/agent1.py\" # 팀 1의 첫 번째 에이전트 경로\n",
    "agent2 = \"/kaggle/working/simulation/agent2.py\" # 팀 1의 두 번째 에이전트 경로\n",
    "\n",
    "# TEAM 2 - QUESTIONER\n",
    "agent3 = \"/kaggle/working/simulation/agent3.py\" # 팀 2의 질문자 에이전트 기본 경로\n",
    "if GEMMA_AS_QUESTIONER: # GEMMA가 질문자로 활동하는 경우\n",
    "    agent3 = \"/kaggle/working/submission/main.py\" # GEMMA의 질문자 에이전트 경로로 변경\n",
    "    \n",
    "# TEAM 2 - ANSWERER\n",
    "agent4 = \"/kaggle/working/simulation/agent4.py\" # 팀 2의 답변자 에이전트 기본 경로\n",
    "if GEMMA_AS_ANSWERER: # GEMMA가 답변자로 활동하는 경우\n",
    "    agent4 = \"/kaggle/working/submission/main.py\" # GEMMA의 답변자 에이전트 경로로 변경\n",
    "    \n",
    "env.reset()\n",
    "log = env.run([agent1, agent2, agent3, agent4]) # 에이전트들을 실행하고 로그를 기록\n",
    "\n",
    "env.render(mode=\"ipython\", width=600, height=500) # 결과를 IPython 모드로 렌더링, 크기 지정\n",
    "\n",
    "import gc, torch\n",
    "del make, env, log # 사용한 리소스 삭제\n",
    "gc.collect() # 가비지 컬렉션 실행\n",
    "torch.cuda.empty_cache() # PyTorch CUDA 캐시를 비움"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-21T04:25:58.216169Z",
     "iopub.execute_input": "2024-07-21T04:25:58.216656Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n#########################\n### Round 1\n#########################\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 0, 'questions': [], 'guesses': [], 'answers': [], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\nOUTPUT = 'Is it Duct tape?'\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "074464a3b24442aca1d0302e151c7809"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "2024-07-21 04:26:35.933292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 04:26:35.933390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 04:26:36.075130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Initializing model\nresponse='Is it a thing?'\n\nTeam 1 - Answerer - Agent Random\nINPUT = {'remainingOverageTime': 300, 'questions': ['Is it Duct tape?'], 'guesses': [], 'answers': [], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'miami florida', 'category': 'place', 'step': 1}\nOUTPUT = 'no'\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c78f7bea9fb4660a54bc60118d73e71"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Initializing model\nresponse='no'\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 2, 'questions': ['Is it Duct tape?'], 'guesses': [], 'answers': ['no'], 'role': 'guesser', 'turnType': 'guess', 'keyword': '', 'category': ''}\nOUTPUT = 'Ointment'\nresponse='no'\n\n#########################\n### Round 2\n#########################\n\nTeam 1 - Questioner - Agent Random\nINPUT = {'remainingOverageTime': 300, 'step': 3, 'questions': ['Is it Duct tape?'], 'guesses': ['Ointment'], 'answers': ['no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\nOUTPUT = 'Is it wellington new zealand?'\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
