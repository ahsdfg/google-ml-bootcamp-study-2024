{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM 20 Questions Rule Based LLAMA3 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Set accelerator to GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-24T03:36:29.569877Z",
     "iopub.status.busy": "2024-07-24T03:36:29.569513Z",
     "iopub.status.idle": "2024-07-24T03:37:16.609692Z",
     "shell.execute_reply": "2024-07-24T03:37:16.608675Z",
     "shell.execute_reply.started": "2024-07-24T03:36:29.569848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "### HuggingFace Login\n",
    "\n",
    "Add HugginFace access token to secrets. You can find it in `Add-ons -> secrets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.612002Z",
     "iopub.status.busy": "2024-07-24T03:37:16.611693Z",
     "iopub.status.idle": "2024-07-24T03:37:16.730352Z",
     "shell.execute_reply": "2024-07-24T03:37:16.729307Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.611976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model via HuggingFace\n",
    "To reduce disk usage, download model in `/tmp/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.731872Z",
     "iopub.status.busy": "2024-07-24T03:37:16.731531Z",
     "iopub.status.idle": "2024-07-24T03:38:17.881629Z",
     "shell.execute_reply": "2024-07-24T03:38:17.880505Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.731846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"abacusai/Llama-3-Smaug-8B\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:17.884461Z",
     "iopub.status.busy": "2024-07-24T03:38:17.883827Z",
     "iopub.status.idle": "2024-07-24T03:38:19.057551Z",
     "shell.execute_reply": "2024-07-24T03:38:19.056424Z",
     "shell.execute_reply.started": "2024-07-24T03:38:17.884417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save quantized model\n",
    "Now, load downloaded model on memory with quantization.  \n",
    "This will save storage.\n",
    "\n",
    "\n",
    "Moreover, since the saved model has already been quantized, we do not need `bitsandbytes` package in `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:19.059437Z",
     "iopub.status.busy": "2024-07-24T03:38:19.059083Z",
     "iopub.status.idle": "2024-07-24T03:38:57.277097Z",
     "shell.execute_reply": "2024-07-24T03:38:57.275976Z",
     "shell.execute_reply.started": "2024-07-24T03:38:19.059408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load model on memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:57.278974Z",
     "iopub.status.busy": "2024-07-24T03:38:57.278493Z",
     "iopub.status.idle": "2024-07-24T03:39:13.179699Z",
     "shell.execute_reply": "2024-07-24T03:39:13.178609Z",
     "shell.execute_reply.started": "2024-07-24T03:38:57.278946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save model in submission directory\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:13.182210Z",
     "iopub.status.busy": "2024-07-24T03:39:13.181274Z",
     "iopub.status.idle": "2024-07-24T03:39:13.578283Z",
     "shell.execute_reply": "2024-07-24T03:39:13.577210Z",
     "shell.execute_reply.started": "2024-07-24T03:39:13.182169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unload model from memory\n",
    "import gc, torch\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "#### BasicQuestions\n",
    "In this notebook, category will be specified in system prompt after first question.  \n",
    "So, `BasicQuestions` has one question to determine category.\n",
    "\n",
    "#### PlaceQuestions\n",
    "PlaceQuestions has 3 distinct questions to categorize.  \n",
    "Once answer is 'yes', disable hard-coded questions.\n",
    "\n",
    "#### ThingQuestions\n",
    "ThingQuestions has 3 questions.\n",
    "All questions will be asked in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/questions.py\n",
    "\n",
    "BasicQuestions = [\n",
    "    \"is it a place?\",\n",
    "    # \"is it a thing?\",\n",
    "]\n",
    "\n",
    "PlaceQuestions = [\n",
    "    \"is it a country?\",\n",
    "    \"is it a city?\",\n",
    "    \"is it a natural feature?\",\n",
    "    # \"is it a mountain?\",\n",
    "    # \"is it a river?\",\n",
    "]\n",
    "\n",
    "ThingsQuestions = [\n",
    "    \"is it a living thing?\",   \n",
    "    \"is it edible?\",           \n",
    "    \"is it something that can be held in your hand?\",\n",
    "    \"Does it require electricity to operate?\",\n",
    "    # \"Would the keyword be included in the broad category of [Group]?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/rulebased.py\n",
    "\n",
    "from questions import *\n",
    "\n",
    "\n",
    "class RuleBasedQuestions:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            log (list): A list to store the user's answers.\n",
    "            count (int): The count of questions asked.\n",
    "            enabled (bool): Indicates if all questions have been asked.\n",
    "            category (str): The current category of questions.\n",
    "        \"\"\"\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.enabled = True\n",
    "        self.category = \"basic\"\n",
    "\n",
    "    def getQuestion(self):\n",
    "        \"\"\"\n",
    "        Returns the next question based on the current state of the game.\n",
    "\n",
    "        Returns:\n",
    "            str: The next question to be asked.\n",
    "        \"\"\"\n",
    "        if self.enabled == False:\n",
    "            return \"No more available questions.\"\n",
    "        if self.count < len(BasicQuestions):  # Basic\n",
    "            return BasicQuestions[self.count]\n",
    "        elif self.category == \"place\":  # Place\n",
    "            return PlaceQuestions[self.count - len(BasicQuestions)]\n",
    "        elif self.category == \"things\":  # Thing\n",
    "            return ThingsQuestions[self.count - len(BasicQuestions)]\n",
    "\n",
    "    def logAnswer(self, answer):\n",
    "        \"\"\"\n",
    "        Logs the user's answer and updates the category and count based on the answer.\n",
    "\n",
    "        Parameters:\n",
    "        - answer (str): The user's answer, either \"yes\" or \"no\".\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        answer_yes = True\n",
    "        if \"no\" in answer.lower():\n",
    "            answer_yes = False\n",
    "        self.log.append(answer_yes)\n",
    "\n",
    "        # determine the category by first answer\n",
    "        if self.count == 0:\n",
    "            self.category = \"place\" if answer_yes else \"things\"\n",
    "        self.count += 1\n",
    "\n",
    "        if self.count <= len(BasicQuestions):  # Basic\n",
    "            pass\n",
    "        elif self.category == \"place\":  # Place\n",
    "            if answer_yes or self.count == len(BasicQuestions) + len(PlaceQuestions):  #\n",
    "                self.enabled = True\n",
    "\n",
    "        elif self.category == \"things\":  # Thing\n",
    "            if self.count == len(BasicQuestions) + len(ThingsQuestions):\n",
    "                self.enabled = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.enabled = False\n",
    "        self.category = \"basic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Prompts are reffered from the [Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library/library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_sys_prompt(category):\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to ask questions to the user to guess the word the user is thinking of.\n",
    "The keyword is of category: \"{category}\"\n",
    "Narrow down the possibilities by asking yes/no questions.\n",
    "Think step by step and try to ask the most informative questions.\n",
    "\\n\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def guesser_sys_prompt(category):\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to guess the word the user is thinking of.\n",
    "The keyword is of category: \"{category}\"\n",
    "Think step by step.\n",
    "\\n\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def answerer_sys_prompt(keyword, category):\n",
    "    prompt = f\"\"\"ou are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to answer the questions of the user to help him guess the word you're thinking of.\n",
    "Your answers must be 'yes' or 'no'.\n",
    "The keyword is: \"{keyword}\", it is of category: \"{category}\"\n",
    "Provide accurate answers to help the user to guess the keyword.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Rule based mode is set to True by default.  \n",
    "If you want to disable rule based mode, \n",
    "uncomment `self.RuleBasedAgent.enabled = False` in `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:40.048046Z",
     "iopub.status.busy": "2024-07-24T03:59:40.047609Z",
     "iopub.status.idle": "2024-07-24T03:59:40.058887Z",
     "shell.execute_reply": "2024-07-24T03:59:40.057904Z",
     "shell.execute_reply.started": "2024-07-24T03:59:40.047993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# comment magic command before simulation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from prompts import *\n",
    "from rulebased import RuleBasedQuestions\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids, max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        self.RuleBasedAgent = RuleBasedQuestions()\n",
    "        \n",
    "        # To disable the rule-based agent, uncomment the following line\n",
    "        # self.RuleBasedAgent.enabled = False\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # launch the guesser role\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        if self.RuleBasedAgent.enabled:\n",
    "            question = self.RuleBasedAgent.getQuestion()\n",
    "            return question\n",
    "\n",
    "        ask_prompt = asker_sys_prompt(self.RuleBasedAgent.category)\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.questions) >= 1:\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                chat_template += (\n",
    "                    f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                )\n",
    "                chat_template += (\n",
    "                    f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                )\n",
    "\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        if self.RuleBasedAgent.enabled:\n",
    "            self.RuleBasedAgent.logAnswer(obs.answers[-1])\n",
    "\n",
    "        conv = \"\"\n",
    "        for q, a in zip(obs.questions, obs.answers):\n",
    "            conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "        guess_prompt = (\n",
    "            guesser_sys_prompt(self.RuleBasedAgent.category)\n",
    "            + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n",
    "        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "        )\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{answerer_sys_prompt(obs.keyword, obs.category)}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers) >= 1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += (\n",
    "                    f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                )\n",
    "                chat_template += (\n",
    "                    f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                )\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)\n",
    "\n",
    "    if response == None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:18.800659Z",
     "iopub.status.busy": "2024-07-24T03:54:18.800218Z",
     "iopub.status.idle": "2024-07-24T03:54:35.726272Z",
     "shell.execute_reply": "2024-07-24T03:54:35.725098Z",
     "shell.execute_reply.started": "2024-07-24T03:54:18.800628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run game, you need to specify agent. Before execute next cell, excute main.py cell above with commenting `%%writefile -a submission/main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:35.729109Z",
     "iopub.status.busy": "2024-07-24T03:54:35.728748Z",
     "iopub.status.idle": "2024-07-24T03:57:57.075552Z",
     "shell.execute_reply": "2024-07-24T03:57:57.074453Z",
     "shell.execute_reply.started": "2024-07-24T03:54:35.729080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# game_output = env.run(agents=[agent, agent, agent, agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:57:57.077693Z",
     "iopub.status.busy": "2024-07-24T03:57:57.076802Z",
     "iopub.status.idle": "2024-07-24T03:57:57.152952Z",
     "shell.execute_reply": "2024-07-24T03:57:57.151984Z",
     "shell.execute_reply.started": "2024-07-24T03:57:57.077660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:20.205260Z",
     "iopub.status.busy": "2024-07-24T03:39:20.204918Z",
     "iopub.status.idle": "2024-07-24T03:39:31.595446Z",
     "shell.execute_reply": "2024-07-24T03:39:31.594100Z",
     "shell.execute_reply.started": "2024-07-24T03:39:20.205232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:51.292752Z",
     "iopub.status.busy": "2024-07-24T03:59:51.292086Z",
     "iopub.status.idle": "2024-07-24T04:01:41.448825Z",
     "shell.execute_reply": "2024-07-24T04:01:41.447529Z",
     "shell.execute_reply.started": "2024-07-24T03:59:51.292718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
