{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%bash\nmkdir -p /kaggle/working/submission\nmkdir -p /tmp/model\npip install -q bitsandbytes accelerate\npip install -qU transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T16:26:23.248406Z","iopub.execute_input":"2024-07-29T16:26:23.248658Z","iopub.status.idle":"2024-07-29T16:27:46.968657Z","shell.execute_reply.started":"2024-07-29T16:26:23.248635Z","shell.execute_reply":"2024-07-29T16:27:46.967680Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\n\nHF_TOKEN: str | None  = None\n\ntry:\n    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:27:46.970456Z","iopub.execute_input":"2024-07-29T16:27:46.971022Z","iopub.status.idle":"2024-07-29T16:27:47.066099Z","shell.execute_reply.started":"2024-07-29T16:27:46.970994Z","shell.execute_reply":"2024-07-29T16:27:47.065209Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nfrom pathlib import Path\nimport shutil\n\ng_model_path = Path(\"/tmp/model\")\nif g_model_path.exists():\n    shutil.rmtree(g_model_path)\ng_model_path.mkdir(parents=True)\n\nsnapshot_download(\n    repo_id=\"abacusai/Llama-3-Smaug-8B\",\n    ignore_patterns=\"original*\",\n    local_dir=g_model_path,\n    token=globals().get(\"HF_TOKEN\", None)\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:27:47.067269Z","iopub.execute_input":"2024-07-29T16:27:47.067545Z","iopub.status.idle":"2024-07-29T16:28:43.972143Z","shell.execute_reply.started":"2024-07-29T16:27:47.067521Z","shell.execute_reply":"2024-07-29T16:28:43.971223Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3caca7419c74f58a6686c5e7b1a7537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e9d9fd1b644924b00913dc91cab39a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94cb6ae908f04b19b905af608687c53b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23fb859d37714dd0b069c8f6097a8ace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c50a6baea64d35b398daa8efa68f89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"678df390af634489b8dcbea46d01d067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a884063fc0484fd1a4230c705b7554fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c87faf99ad34d03bfeb54cec58f8f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618253c13abb41528003c211ab03a70c"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/tmp/model'"},"metadata":{}}]},{"cell_type":"code","source":"!ls -l /tmp/model","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:43.974901Z","iopub.execute_input":"2024-07-29T16:28:43.975460Z","iopub.status.idle":"2024-07-29T16:28:44.994743Z","shell.execute_reply.started":"2024-07-29T16:28:43.975425Z","shell.execute_reply":"2024-07-29T16:28:44.993805Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"total 15693124\n-rw-r--r-- 1 root root       1728 Jul 29 16:27 README.md\n-rw-r--r-- 1 root root        649 Jul 29 16:27 config.json\n-rw-r--r-- 1 root root        121 Jul 29 16:27 generation_config.json\n-rw-r--r-- 1 root root 4976698672 Jul 29 16:28 model-00001-of-00004.safetensors\n-rw-r--r-- 1 root root 4999802720 Jul 29 16:28 model-00002-of-00004.safetensors\n-rw-r--r-- 1 root root 4915916176 Jul 29 16:28 model-00003-of-00004.safetensors\n-rw-r--r-- 1 root root 1168138808 Jul 29 16:27 model-00004-of-00004.safetensors\n-rw-r--r-- 1 root root      23950 Jul 29 16:27 model.safetensors.index.json\n-rw-r--r-- 1 root root        449 Jul 29 16:27 special_tokens_map.json\n-rw-r--r-- 1 root root    9084463 Jul 29 16:27 tokenizer.json\n-rw-r--r-- 1 root root      51016 Jul 29 16:27 tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# load model on memory\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\ndownloaded_model = \"/tmp/model\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    downloaded_model,\n    quantization_config = bnb_config,\n    torch_dtype = torch.float16,\n    device_map = \"auto\",\n    trust_remote_code = True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(downloaded_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:44.996118Z","iopub.execute_input":"2024-07-29T16:28:44.996396Z","iopub.status.idle":"2024-07-29T16:29:22.411232Z","shell.execute_reply.started":"2024-07-29T16:28:44.996371Z","shell.execute_reply":"2024-07-29T16:29:22.410230Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3311b4c5bf34549a61fc5cfa7c5a251"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb5913a8f784622a3bce41c9cdcebe5"}},"metadata":{}}]},{"cell_type":"code","source":"# save model in submission directory\nmodel.save_pretrained(\"/kaggle/working/submission/model\")\ntokenizer.save_pretrained(\"/kaggle/working/submission/model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:22.412781Z","iopub.execute_input":"2024-07-29T16:29:22.413368Z","iopub.status.idle":"2024-07-29T16:29:37.799753Z","shell.execute_reply.started":"2024-07-29T16:29:22.413332Z","shell.execute_reply":"2024-07-29T16:29:37.798773Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/submission/model/tokenizer_config.json',\n '/kaggle/working/submission/model/special_tokens_map.json',\n '/kaggle/working/submission/model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# unload model from memory\nimport gc, torch\ndel model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:37.800967Z","iopub.execute_input":"2024-07-29T16:29:37.801375Z","iopub.status.idle":"2024-07-29T16:29:38.159906Z","shell.execute_reply.started":"2024-07-29T16:29:37.801350Z","shell.execute_reply":"2024-07-29T16:29:38.159045Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%writefile submission/questions.py\n\nBasicQuestions = [\n    \"is it a place?\",\n    # \"is it a thing?\",\n]\n\nPlaceQuestions = [\n    \"is it a country?\",\n    \"is it a city?\",\n    \"is it a natural feature?\",\n    # \"is it a mountain?\",\n    # \"is it a river?\",\n]\n\nThingsQuestions = [\n    \"is it a living thing?\",   \n    \"is it edible?\",           \n    \"is it something that can be held in your hand?\",\n    \"Does it require electricity to operate?\",\n    # \"Would the keyword be included in the broad category of [Group]?\",\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:38.170244Z","iopub.execute_input":"2024-07-29T16:29:38.170674Z","iopub.status.idle":"2024-07-29T16:29:38.971011Z","shell.execute_reply.started":"2024-07-29T16:29:38.170637Z","shell.execute_reply":"2024-07-29T16:29:38.969791Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing submission/questions.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile submission/rulebased.py\n\nfrom questions import *\n\n\nclass RuleBasedQuestions:\n    def __init__(self):\n        \"\"\"\n        Attributes:\n            log (list): A list to store the user's answers.\n            count (int): The count of questions asked.\n            enabled (bool): Indicates if all questions have been asked.\n            category (str): The current category of questions.\n        \"\"\"\n        self.log = []\n        self.count = 0\n        self.enabled = True\n        self.category = \"basic\"\n\n    def getQuestion(self):\n        \"\"\"\n        Returns the next question based on the current state of the game.\n\n        Returns:\n            str: The next question to be asked.\n        \"\"\"\n        if self.enabled == False:\n            return \"No more available questions.\"\n        if self.category == \"basic\":\n            return BasicQuestions[self.count]\n        elif self.category == \"place\":\n            return PlaceQuestions[self.count - len(BasicQuestions)]\n        elif self.category == \"things\":\n            return ThingsQuestions[self.count - len(BasicQuestions)]\n\n    def logAnswer(self, answer):\n        \"\"\"\n        Logs the user's answer and updates the category and count based on the answer.\n\n        Parameters:\n        - answer (str): The user's answer, either \"yes\" or \"no\".\n\n        Returns:\n        None\n        \"\"\"\n        answer_yes = True\n        if \"no\" in answer.lower():\n            answer_yes = False\n        self.log.append(answer_yes)\n\n        # determine the category by first answer\n        if self.count == 0:\n            self.category = \"place\" if answer_yes else \"things\"\n        self.count += 1\n\n        if self.category == \"basic\": \n            pass\n        elif self.category == \"place\":\n            if answer_yes or self.count == len(BasicQuestions) + len(PlaceQuestions):\n                self.enabled = False\n        elif self.category == \"things\":\n            if self.count == len(BasicQuestions) + len(ThingsQuestions):\n                self.enabled = False\n\n    def reset(self):\n        self.log = []\n        self.count = 0\n        self.enabled = True\n        self.category = \"basic\"","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:10.789848Z","iopub.execute_input":"2024-07-29T16:32:10.790216Z","iopub.status.idle":"2024-07-29T16:32:10.797851Z","shell.execute_reply.started":"2024-07-29T16:32:10.790188Z","shell.execute_reply":"2024-07-29T16:32:10.796947Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Writing submission/rulebased.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile submission/prompts.py\n\ndef asker_sys_prompt(category):\n    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\nYour task is to ask questions to the user to guess the word the user is thinking of.\nThe keyword is of category: \"{category}\"\nNarrow down the possibilities by asking yes/no questions.\nThink step by step and try to ask the most informative questions.\n\\n\"\"\"\n    return prompt\n\n\ndef guesser_sys_prompt(category):\n    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\nYour task is to guess the word the user is thinking of.\nThe keyword is of category: \"{category}\"\nThink step by step.\n\\n\"\"\"\n    return prompt\n\n\ndef answerer_sys_prompt(keyword, category):\n    prompt = f\"\"\"ou are a helpful AI assistant with expertise in playing 20 questions game.\nYour task is to answer the questions of the user to help him guess the word you're thinking of.\nYour answers must be 'yes' or 'no'.\nThe keyword is: \"{keyword}\", it is of category: \"{category}\"\nProvide accurate answers to help the user to guess the keyword.\n\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:17.907576Z","iopub.execute_input":"2024-07-29T16:32:17.908198Z","iopub.status.idle":"2024-07-29T16:32:17.914545Z","shell.execute_reply.started":"2024-07-29T16:32:17.908167Z","shell.execute_reply":"2024-07-29T16:32:17.913496Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Writing submission/prompts.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile submission/main.py\n# comment magic command before simulation\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nimport sys\n\nfrom prompts import *\nfrom rulebased import RuleBasedQuestions\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\n\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\nelse:\n    MODEL_PATH = \"/kaggle/working/submission/model\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nid_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n\n\ndef generate_answer(template):\n    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n    out_ids = model.generate(**inp_ids, max_new_tokens=15).squeeze()\n    start_gen = inp_ids.input_ids.shape[1]\n    out_ids = out_ids[start_gen:]\n    if id_eot in out_ids:\n        stop = out_ids.tolist().index(id_eot)\n        out = tokenizer.decode(out_ids[:stop])\n    else:\n        out = tokenizer.decode(out_ids)\n    return out\n\n\nclass Robot:\n    def __init__(self):\n        self.RuleBasedAgent = RuleBasedQuestions()\n        \n        # To disable the rule-based agent, uncomment the following line\n        # self.RuleBasedAgent.enabled = False\n\n    def on(self, mode, obs):\n        assert mode in [\n            \"asking\", \"guessing\", \"answering\",\n        ], \"mode can only take one of these values: asking, answering, guessing\"\n        if mode == \"asking\":\n            # launch the asker role\n            output = self.asker(obs)\n        if mode == \"answering\":\n            # launch the answerer role\n            output = self.answerer(obs)\n            if \"yes\" in output.lower():\n                output = \"yes\"\n            elif \"no\" in output.lower():\n                output = \"no\"\n            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n                output = \"yes\"\n        if mode == \"guessing\":\n            # launch the guesser role\n            output = self.guesser(obs)\n        return output\n\n    def asker(self, obs):\n        if self.RuleBasedAgent.enabled:\n            question = self.RuleBasedAgent.getQuestion()\n            return question\n\n        ask_prompt = asker_sys_prompt(self.RuleBasedAgent.category)\n        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        if len(obs.questions) >= 1:\n            for q, a in zip(obs.questions, obs.answers):\n                chat_template += (\n                    f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n                )\n                chat_template += (\n                    f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n                )\n\n        output = generate_answer(chat_template)\n        return output\n\n    def guesser(self, obs):\n        if self.RuleBasedAgent.enabled:\n            self.RuleBasedAgent.logAnswer(obs.answers[-1])\n\n        conv = \"\"\n        for q, a in zip(obs.questions, obs.answers):\n            conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n        guess_prompt = (\n            guesser_sys_prompt(self.RuleBasedAgent.category)\n            + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n        )\n        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n        output = generate_answer(chat_template)\n        return output\n\n    def answerer(self, obs):\n\n        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{answerer_sys_prompt(obs.keyword, obs.category)}<|eot_id|>\"\"\"\n        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        if len(obs.answers) >= 1:\n            for q, a in zip(obs.questions[1:], obs.answers):\n                chat_template += (\n                    f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n                )\n                chat_template += (\n                    f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n                )\n        output = generate_answer(chat_template)\n        return output\n\n\nrobot = Robot()\n\n\ndef agent(obs, cfg):\n\n    if obs.turnType == \"ask\":\n        response = robot.on(mode=\"asking\", obs=obs)\n\n    elif obs.turnType == \"guess\":\n        response = robot.on(mode=\"guessing\", obs=obs)\n\n    elif obs.turnType == \"answer\":\n        response = robot.on(mode=\"answering\", obs=obs)\n\n    if response == None or len(response) <= 1:\n        response = \"yes\"\n\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:24.998602Z","iopub.execute_input":"2024-07-29T16:32:24.998977Z","iopub.status.idle":"2024-07-29T16:32:25.008751Z","shell.execute_reply.started":"2024-07-29T16:32:24.998947Z","shell.execute_reply":"2024-07-29T16:32:25.007691Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Writing submission/main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install pigz pv > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:30.163602Z","iopub.execute_input":"2024-07-29T16:32:30.164294Z","iopub.status.idle":"2024-07-29T16:32:36.869935Z","shell.execute_reply.started":"2024-07-29T16:32:30.164259Z","shell.execute_reply":"2024-07-29T16:32:36.868632Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:37.049170Z","iopub.execute_input":"2024-07-29T16:32:37.049917Z","iopub.status.idle":"2024-07-29T16:34:13.681925Z","shell.execute_reply.started":"2024-07-29T16:32:37.049869Z","shell.execute_reply":"2024-07-29T16:34:13.680858Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"4.73GiB 0:01:35 [50.7MiB/s] [   <=>                                            ]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}