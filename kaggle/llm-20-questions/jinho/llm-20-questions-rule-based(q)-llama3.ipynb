{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set accelerator as GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-24T03:36:29.569877Z",
     "iopub.status.busy": "2024-07-24T03:36:29.569513Z",
     "iopub.status.idle": "2024-07-24T03:37:16.609692Z",
     "shell.execute_reply": "2024-07-24T03:37:16.608675Z",
     "shell.execute_reply.started": "2024-07-24T03:36:29.569848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "### HuggingFace Login\n",
    "\n",
    "Add HugginFace access token to secrets. You can find it in `Add-ons -> secrets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.612002Z",
     "iopub.status.busy": "2024-07-24T03:37:16.611693Z",
     "iopub.status.idle": "2024-07-24T03:37:16.730352Z",
     "shell.execute_reply": "2024-07-24T03:37:16.729307Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.611976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model via HuggingFace\n",
    "To reduce disk usage, download model in `/tmp/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.731872Z",
     "iopub.status.busy": "2024-07-24T03:37:16.731531Z",
     "iopub.status.idle": "2024-07-24T03:38:17.881629Z",
     "shell.execute_reply": "2024-07-24T03:38:17.880505Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.731846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"abacusai/Llama-3-Smaug-8B\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:17.884461Z",
     "iopub.status.busy": "2024-07-24T03:38:17.883827Z",
     "iopub.status.idle": "2024-07-24T03:38:19.057551Z",
     "shell.execute_reply": "2024-07-24T03:38:19.056424Z",
     "shell.execute_reply.started": "2024-07-24T03:38:17.884417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save quantized model\n",
    "Now, load downloaded model on memory with quantization.\n",
    "This will save storage.\n",
    "Moreover, since the saved model has already been quantized, we do not need `bitsandbytes` package in `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:19.059437Z",
     "iopub.status.busy": "2024-07-24T03:38:19.059083Z",
     "iopub.status.idle": "2024-07-24T03:38:57.277097Z",
     "shell.execute_reply": "2024-07-24T03:38:57.275976Z",
     "shell.execute_reply.started": "2024-07-24T03:38:19.059408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load model on memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:57.278974Z",
     "iopub.status.busy": "2024-07-24T03:38:57.278493Z",
     "iopub.status.idle": "2024-07-24T03:39:13.179699Z",
     "shell.execute_reply": "2024-07-24T03:39:13.178609Z",
     "shell.execute_reply.started": "2024-07-24T03:38:57.278946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save model in submission directory\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:13.182210Z",
     "iopub.status.busy": "2024-07-24T03:39:13.181274Z",
     "iopub.status.idle": "2024-07-24T03:39:13.578283Z",
     "shell.execute_reply": "2024-07-24T03:39:13.577210Z",
     "shell.execute_reply.started": "2024-07-24T03:39:13.182169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unload model from memory\n",
    "import gc, torch\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/questions.py\n",
    "\n",
    "BasicQuestions = [\n",
    "    \"is it a place?\",\n",
    "    \"is it a thing?\",\n",
    "]\n",
    "\n",
    "PlaceQuestions = [\n",
    "    \"is the place a country?\",\n",
    "    \"is the place a city?\",\n",
    "    \"is the place a mountain?\",\n",
    "    \"is the place a river?\",\n",
    "]\n",
    "\n",
    "ThingsQuestions = [\n",
    "    \"is the thing a living thing?\",   \n",
    "    \"is the thing edible?\",           \n",
    "    \"is the thing something that can be held in your hand?\",\n",
    "    \"Does the thing require electricity to operate?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "asker_sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "the user is going to think of a word, it can be only one of the following 2 categories:\n",
    "1. a place\n",
    "2. a thing\n",
    "So focus your area of search on these options. and give smart questions that narrows down the search space\\n\"\"\"\n",
    "\n",
    "asker_one_shot_prompt = \"\"\"your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
    "            to help you, here's an example of how it should work assuming that the keyword is seoul in the category \"place\":\n",
    "            example:\n",
    "            <assistant: is the keyword a place?\n",
    "            user: yes\n",
    "            assistant: is the keyword a country?\n",
    "            user: no\n",
    "            assistant: is the keyword a city?\n",
    "            user: yes\n",
    "            assistant: is the city in europe?\n",
    "            user: no\n",
    "            assistant: is the city in asia?\n",
    "            user: yes\n",
    "            assistant: is the city name starting by s ?\n",
    "            user: yes\n",
    "            assistant: is the city seoul?\n",
    "            user: yes>\n",
    "\n",
    "            the user has chosen the word, ask your first question!\n",
    "            please be short and not verbose, give only one question, no extra word!\"\"\"\n",
    "\n",
    "guesser_sys_prompt = asker_sys_prompt\n",
    "\n",
    "def answerer_sys_prompt(keyword, category):\n",
    "    prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n",
    "Know that the user will always guess a word belonging to one of the following 2 categories:\n",
    "1. a place\n",
    "2. a thing\n",
    "so make sure you understand the user's question and you understand the keyword you're playig on.\n",
    "for now the word that the user should guess is: \"{keyword}\", it is of category \"{category}\",\n",
    "to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n",
    "example:\n",
    "<user: is it a place?\n",
    "assistant: yes\n",
    "user: is it in europe?\n",
    "assistant: no\n",
    "user: is it in africa?\n",
    "assistant: yes\n",
    "user: do most people living there have dark skin?\n",
    "assistant: no\n",
    "user: is it a country name starting by m ?\n",
    "assistant: yes\n",
    "user: is it Morocco?\n",
    "assistant: yes>\"\"\"\n",
    "    \n",
    "    return prompt   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/rulebased.py\n",
    "\n",
    "from questions import *\n",
    "\n",
    "class RuleBasedQuestions:\n",
    "    \"\"\"\n",
    "    Rule-based 20 Questions\n",
    "    \n",
    "    1. Basic Questions\n",
    "\n",
    "    2. Place or Thing Questions\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the log, count, and done.\n",
    "        \"\"\"\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def getQuestion(self):\n",
    "        \"\"\"\n",
    "        Get the next question. Return the question based on the count.\n",
    "        If there is no more hard-coded question, return \"No more available questions.\"\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return \"No more available questions.\"\n",
    "        if self.count < len(BasicQuestions):          # Basic\n",
    "            return BasicQuestions[self.count]\n",
    "        elif self.log[0] == True:   # Place\n",
    "            return PlaceQuestions[self.count-len(BasicQuestions)]\n",
    "        else:                       # Thing\n",
    "            return ThingsQuestions[self.count-len(BasicQuestions)]\n",
    "        \n",
    "    def logAnswer(self, answer):\n",
    "        \"\"\"\n",
    "        Put the answer to the question.\n",
    "        \"\"\"\n",
    "        \n",
    "        # parse \"yes\" or \"no\" from the answer\n",
    "        answer_yes = True\n",
    "        if \"no\" in answer.lower():\n",
    "            answer_yes = False\n",
    "            \n",
    "        self.log.append(answer_yes)\n",
    "        self.count += 1\n",
    "        \n",
    "        if self.count <= len(BasicQuestions):          # Basic\n",
    "            pass\n",
    "        elif self.log[0] == True:   # Place\n",
    "            if answer_yes or self.count == len(BasicQuestions) + len(PlaceQuestions):\n",
    "                self.done = True\n",
    "        else:                       # Thing\n",
    "            if self.count == len(BasicQuestions) + len(ThingsQuestions):\n",
    "                self.done = True\n",
    "            \n",
    "\n",
    "    def reset(self):\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:40.048046Z",
     "iopub.status.busy": "2024-07-24T03:59:40.047609Z",
     "iopub.status.idle": "2024-07-24T03:59:40.058887Z",
     "shell.execute_reply": "2024-07-24T03:59:40.057904Z",
     "shell.execute_reply.started": "2024-07-24T03:59:40.047993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# comment magic command before simulation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from prompts import *\n",
    "from rulebased import RuleBasedQuestions\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    " \n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        self.RuleBasedAgent = RuleBasedQuestions()\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            #launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            #launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"   \n",
    "            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            #launch the guesser role\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        if self.RuleBasedAgent.done == False:\n",
    "            question = self.RuleBasedAgent.getQuestion()\n",
    "            return question\n",
    "            \n",
    "        ask_prompt = asker_sys_prompt + asker_one_shot_prompt\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.questions)>=1:\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "    def guesser(self, obs):\n",
    "        if self.RuleBasedAgent.done == False:\n",
    "            self.RuleBasedAgent.logAnswer(obs.answers[-1])\n",
    "        \n",
    "        conv = \"\"\n",
    "        for q, a in zip(obs.questions, obs.answers):\n",
    "            conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "        guess_prompt =  guesser_sys_prompt + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n",
    "        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "    def answerer(self, obs):\n",
    "\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{answerer_sys_prompt(obs.keyword, obs.category)}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:18.800659Z",
     "iopub.status.busy": "2024-07-24T03:54:18.800218Z",
     "iopub.status.idle": "2024-07-24T03:54:35.726272Z",
     "shell.execute_reply": "2024-07-24T03:54:35.725098Z",
     "shell.execute_reply.started": "2024-07-24T03:54:18.800628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run game, you need to specify agent. Before execute next cell, excute main.py cell above with commenting `%%writefile -a submission/main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:35.729109Z",
     "iopub.status.busy": "2024-07-24T03:54:35.728748Z",
     "iopub.status.idle": "2024-07-24T03:57:57.075552Z",
     "shell.execute_reply": "2024-07-24T03:57:57.074453Z",
     "shell.execute_reply.started": "2024-07-24T03:54:35.729080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# game_output = env.run(agents=[agent, agent, agent, agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:57:57.077693Z",
     "iopub.status.busy": "2024-07-24T03:57:57.076802Z",
     "iopub.status.idle": "2024-07-24T03:57:57.152952Z",
     "shell.execute_reply": "2024-07-24T03:57:57.151984Z",
     "shell.execute_reply.started": "2024-07-24T03:57:57.077660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:20.205260Z",
     "iopub.status.busy": "2024-07-24T03:39:20.204918Z",
     "iopub.status.idle": "2024-07-24T03:39:31.595446Z",
     "shell.execute_reply": "2024-07-24T03:39:31.594100Z",
     "shell.execute_reply.started": "2024-07-24T03:39:20.205232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:51.292752Z",
     "iopub.status.busy": "2024-07-24T03:59:51.292086Z",
     "iopub.status.idle": "2024-07-24T04:01:41.448825Z",
     "shell.execute_reply": "2024-07-24T04:01:41.447529Z",
     "shell.execute_reply.started": "2024-07-24T03:59:51.292718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
