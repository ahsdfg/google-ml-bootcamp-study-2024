{\rtf1\ansi\ansicpg949\cocoartf2638
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset129 AppleSDGothicNeo-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 NLP Paper\
\
<Word Embedding>\
Word2Vec : Efficient Estimation of Word Representations in Vector Space (2013)\
GloVe : Global Vectors for Word Representation (2014)\
ELMo : Deep contextualized word representations (2018)\
\
<Text Embedding>\
RNN : Recurrent neural network based language model (2010)\
LSTM : Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014)\
GRU : Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation (2014)\
\
<Text Embedding>\
Seq2Seq : Sequence to Sequence Learning with Neural Networks (2014)\
Attention : Neural Machine Translation by Jointly Learning to Align and Translate (2015)\
Transformer : Attention is All You Need (2017)\
\
<Text Embedding>\
BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding (2018)\
RoBERTa : RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019)\
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations (2019)\
\
<Text Regression/Classification>\
MTEB: Massive Text Embedding Benchmark (=> \'ba\'b0\'b7\'ce\'bf\'b4\'c0\'bd \'be\'c8\'ba\'c1\'b5\'b5 \'b5\'c9\'b5\'ed)\
XLNet: Generalized Autoregressive Pretraining for Language Understanding\
\
<Text Generation>\
GPT-1 : Improving Language Understanding by Generative Pre-Training (2018)\
GPT-2 : Language Models are Unsupervised Multitask Learners (2019)\
GPT-3 : Language Models are Few-Shot Learners (2020)\
Training language models to follow instructions with human feedback\
\
<Document Retrieval>\
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\
Self-Knowledge Guided Retrieval Augmentation for Large Language Models\
\
<LLM>\
LLaMA: Open and Efficient Foundation Language Models (2023)\
Mistral 7B (2023)  - \'b0\'b3\'c0\'ce\'c0\'fb\'c0\'b8\'b7\'ce \'c0\'df \'b8\'f0\'b8\'a3\'b0\'da\'c0\'bd\
LoRA: Low-Rank Adaptation of Large Language Models (2021)\
\
\'b0\'b3\'c0\'ce\'c0\'fb\'c0\'b8\'b7\'ce Word2Vec, RNN, LSTM, Seq2Seq, Attention, Transformer, BERT, GPT-1, RAG \'b8\'b8\'c5\'ad\'c0\'ba \'b2\'c0 \'c0\'d0\'be\'ee\'ba\'b8\'bd\'c3\'b8\'e9 \'c1\'c1\'c0\'bb \'b0\'cd \'b0\'b0\'bd\'c0\'b4\'cf\'b4\'d9.\
\'c6\'af\'c8\'f7 Transformer \'b3\'aa \'b1\'d7\'b7\'ce \'c6\'c4\'bb\'fd\'b5\'c8 BERT, GPT\'b4\'c2 \'bf\'e4\'c1\'f2 \'c0\'ce\'b0\'f8\'c1\'f6\'b4\'c9 \'ba\'d5\'c0\'bb \'c0\'cf\'c0\'b8\'c5\'b2 \'c0\'e5\'ba\'bb\'c0\'ce\'b5\'e9\'c0\'cc\'be\'ee\'bc\'ad \'be\'cb\'b0\'ed\'b0\'e8\'bd\'c3\'b8\'e9 \'b3\'aa\'c1\'df\'bf\'a1 \'bf\'a9\'c0\'da\'c4\'a3\'b1\'b8\'c7\'d1\'c5\'d7 \'be\'c6\'b4\'c2 \'c3\'b4\'c7\'cf\'b1\'e2 \'b8\'c5\'bf\'ec \'c1\'c1\'bd\'c0\'b4\'cf\'b4\'d9! (+ \'bf\'ac\'b1\'b8\'bd\'c7 \'b5\'e9\'be\'ee\'b0\'a5 \'b6\'a7 \'c7\'d7\'bb\'f3 \'b9\'b0\'c0\'b8\'bd\'c3\'b4\'c2 \'b0\'b3\'b3\'e4\'b5\'e9\'c0\'cc\'b1\'e2\'b5\'b5 \'c7\'d5\'b4\'cf\'b4\'d9)}